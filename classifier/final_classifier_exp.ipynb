{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to generate the finalized version of the classifier, to simply feature transformation into the final form, and to test that the results are the same\n",
    "\n",
    "Most of the code comes from operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "#Loading raw data\n",
    "# df = pickle.load(open(\"../Data/multiclass_tweets_indexed.p\",'rb'))\n",
    "# df = pickle.load(open(\"../Data/labeled_data.p\",'rb'))\n",
    "df = pd.read_pickle(\"../Data/labeled_data.p\")\n",
    "# tweets = df.text\n",
    "tweets = df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither class  \\\n",
       "0      3            0                   0        3     2   \n",
       "1      3            0                   3        0     1   \n",
       "2      3            0                   3        0     1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3        !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "5        !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
       "6        !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
       "7        !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
       "8        \" &amp; you might not get ya bitch back &amp; ...\n",
       "9        \" @rhythmixx_ :hobbies include: fighting Maria...\n",
       "10       \" Keeks is a bitch she curves everyone \" lol I...\n",
       "11                      \" Murda Gang bitch its Gang Land \"\n",
       "12       \" So hoes that smoke are losers ? \" yea ... go...\n",
       "13           \" bad bitches is the only thing that i like \"\n",
       "14                                 \" bitch get up off me \"\n",
       "15                         \" bitch nigga miss me with it \"\n",
       "16                                  \" bitch plz whatever \"\n",
       "17                               \" bitch who do you love \"\n",
       "18                      \" bitches get cut off everyday B \"\n",
       "19                      \" black bottle &amp; a bad bitch \"\n",
       "20                    \" broke bitch cant tell me nothing \"\n",
       "21                         \" cancel that bitch like Nino \"\n",
       "22                 \" cant you see these hoes wont change \"\n",
       "23       \" fuck no that bitch dont even suck dick \" &#1...\n",
       "24       \" got ya bitch tip toeing on my hardwood floor...\n",
       "25          \" her pussy lips like Heaven doors \" &#128524;\n",
       "26                            \" hoe what its hitting for \"\n",
       "27       \" i met that pussy on Ocean Dr . i gave that p...\n",
       "28          \" i need a trippy bitch who fuck on Hennessy \"\n",
       "29       \" i spend my money how i want bitch its my bus...\n",
       "                               ...                        \n",
       "25266               you ain't gotta be a dyke to like hoes\n",
       "25267                     you are a hoe, hoe, &amp; a hoe.\n",
       "25268               you bitches love yall some corny nigga\n",
       "25269    you can masturbate anytime bitch lol &#8220;@g...\n",
       "25270    you can never get a group of hoes together wit...\n",
       "25271    you can tell when dick recently been in a puss...\n",
       "25272                            you can't cuff a hoe lmao\n",
       "25273                           you drove me redneck crazy\n",
       "25274                                you fake niggah lolol\n",
       "25275                   you got niggas, and i got bitches.\n",
       "25276    you gotta be a new breed of retarded if you do...\n",
       "25277    you gotta understand that these bitches are ch...\n",
       "25278                                        you hoe spice\n",
       "25279                     you just want some attention hoe\n",
       "25280    you know what they say, the early bird gets th...\n",
       "25281    you know what your doing when you favorite a t...\n",
       "25282    you lil dumb ass bitch, i ain't fuckin wit chu...\n",
       "25283    you look like AC Green...bitch don't call here...\n",
       "25284    you look like your 12 stop talking about fucki...\n",
       "25285          you might as well gone pussy pop on a stage\n",
       "25286                you niggers cheat on ya gf's? smh....\n",
       "25287    you really care bout dis bitch. my dick all in...\n",
       "25288     you worried bout other bitches, you need me for?\n",
       "25289                                   you're all niggers\n",
       "25290    you're such a retard i hope you get type 2 dia...\n",
       "25291    you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "25292    you've gone and broke the wrong heart baby, an...\n",
       "25294    young buck wanna eat!!.. dat nigguh like I ain...\n",
       "25295                youu got wild bitches tellin you lies\n",
       "25296    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df.tweet\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "# idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict = {}\n",
    "# for i in vocab.values():\n",
    "#     print(str(i) + '\\t\\t' + str(idf_vals[i]))\n",
    "#     idf_dict[i] = idf_vals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab\n",
    "\n",
    "# for i, v in enumerate(vectorizer.get_feature_names()):\n",
    "#     print(str(i) + '\\t\\t' + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in vocab.values():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "#     print(tags)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        RB NN IN DT NN PRP VBP JJ NN IN VBG RP PRP$ NN...\n",
       "1           JJ NN NN NNS VBP JJ JJ IN NN NN NN IN DT NN NN\n",
       "2        JJ NN NN NN PRP RB VBD DT NN CC PRP VBP TO VB ...\n",
       "3                                  NN VBZ PRP VBP IN DT NN\n",
       "4        NN VBZ DT NN PRP VBP IN PRP MD VB JJ CC PRP MD...\n",
       "5        IN DT NN RB VBZ NN PRP RB JJ CC RB IN NN CC RB...\n",
       "6        NN NN MD RB RB VB RP CC VB IN DT NN NN NN VBD ...\n",
       "7        NN NN NN VBP VBN IN PRP JJ NNS VBG IN PRP VBP NNS\n",
       "8                      NN PRP MD RB VB JJ VB RB JJ NNS WDT\n",
       "9                                       NNS VBP VBG NNS NN\n",
       "10       NN VBZ DT NN PRP VBZ NN NN NN VBD IN DT NN IN ...\n",
       "11                                     NN NN VB PRP$ NN NN\n",
       "12                   RB VBZ IN NN VBP NNS . NN : VBP IN NN\n",
       "13                            JJ NNS VBZ DT JJ NN IN NN IN\n",
       "14                                         NN VB RP IN PRP\n",
       "15                                     NN NN VB PRP IN PRP\n",
       "16                                               NN NN WDT\n",
       "17                                        NN WP VBP PRP VB\n",
       "18                                    NNS VBP VBN RP JJ NN\n",
       "19                                      JJ NN VBD DT JJ NN\n",
       "20                                     VBD NN NN VB PRP NN\n",
       "21                                        NN WDT VBP IN NN\n",
       "22                                NN PRP VBP DT NNS VBP NN\n",
       "23       NN DT WDT VBP VB RB JJ VBP DT NN NN NN TO VB V...\n",
       "24                        VBD PRP VB NN VBG IN PRP$ NN NNS\n",
       "25                                   PRP$ JJ NNS IN NN NNS\n",
       "26                                        VB WP PRP$ NN IN\n",
       "27              NN VBD IN NN IN JJ NN . NN VBD IN NN DT NN\n",
       "28                           NNS VBP DT JJ NN WP VBP IN NN\n",
       "29               JJ VBP PRP$ NN WRB NN VBP NN PRP$ PRP$ NN\n",
       "                               ...                        \n",
       "24753                     PRP VBP JJ NN VB DT NN TO VB NNS\n",
       "24754                           PRP VBP DT JJ NN VBZ DT NN\n",
       "24755                               PRP VBP VB CC DT NN NN\n",
       "24756      PRP MD VB JJ NN NN NN RB VB DT NN RB JJ NNS VBP\n",
       "24757     PRP MD RB VB DT NN IN NNS RB IN NN VBG TO VBG NN\n",
       "24758    PRP MD VB WRB NN RB VBN IN DT JJ NN JJ NN NN N...\n",
       "24759                                PRP MD VB VB DT NN NN\n",
       "24760                                     PRP VB PRP VB JJ\n",
       "24761                                        PRP VBP JJ NN\n",
       "24762                             PRP VBD JJ CC JJ VBD NNS\n",
       "24763              PRP VBP VB DT JJ NN IN JJ IN PRP VBP VB\n",
       "24764                         PRP VBP VBP IN DT NNS VBP JJ\n",
       "24765                                          PRP VBP NNS\n",
       "24766                                  PRP RB VBP DT NN NN\n",
       "24767    PRP VBP WP PRP VBP DT JJ NN VBZ DT NN VBZ JJ N...\n",
       "24768    PRP VBP WP PRP$ VBG WRB PRP VBP DT NN NN NN NN...\n",
       "24769    PRP VBP JJ NN NN NN VBP JJ JJ NN NN NN VBD DT ...\n",
       "24770                    PRP VBP IN NNS VBP VB NN NN RB NN\n",
       "24771                    PRP VBP IN PRP$ NN VBG IN VBG NNS\n",
       "24772                      PRP MD RB RB VBN JJ NN IN DT NN\n",
       "24773                           PRP NNS VBP IN NN NN NN NN\n",
       "24774              PRP RB VB IN NN NN PRP$ NN DT IN JJ NNS\n",
       "24775                      PRP VBP IN JJ NN PRP VBP PRP VB\n",
       "24776                                       PRP VBP DT NNS\n",
       "24777    PRP VBP PDT DT NN NN VBP PRP VBP JJ NNS CC NN ...\n",
       "24778    PRP VBP DT NN IN NN VB PRP$ NN VBZ JJ . JJ NN ...\n",
       "24779       PRP VBP VBN CC VBD DT JJ NN NN CC VB PRP VB JJ\n",
       "24780            JJ NN JJ NN NN IN IN JJ VBP NNS VBP RP RB\n",
       "24781                            NN VBD JJ NNS VBP PRP NNS\n",
       "24782      VBN RB JJ NNS JJ NN NN IN NN NN JJ NN VBD DT NN\n",
       "Length: 24783, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tweet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN NN MD': 1231,\n",
       " 'JJS NNS TO': 1055,\n",
       " 'VBG JJ VBG': 3053,\n",
       " 'NN RP IN': 1336,\n",
       " 'JJ WRB VBZ': 986,\n",
       " 'PRP IN VB': 1822,\n",
       " 'JJ RB TO': 881,\n",
       " 'WP VB PRP': 3921,\n",
       " 'VBP TO': 3490,\n",
       " 'RB JJ VBP': 2133,\n",
       " 'IN NN JJR': 578,\n",
       " 'CD NN NNS': 203,\n",
       " 'EX MD': 446,\n",
       " 'RB TO PRP': 2216,\n",
       " 'VBP VBG VBD': 3537,\n",
       " 'IN DT VBZ': 514,\n",
       " 'PRP VBD VBD': 1984,\n",
       " 'PRP VBN DT': 2003,\n",
       " 'VBD VB': 2961,\n",
       " 'WRB JJ IN': 3980,\n",
       " 'VBZ JJ DT': 3638,\n",
       " 'RBR RB VB': 2349,\n",
       " 'VB DT': 2548,\n",
       " 'VBD CD NN': 2813,\n",
       " 'VB RB VBZ': 2693,\n",
       " 'NN IN VBG': 1182,\n",
       " 'NN WDT MD': 1477,\n",
       " 'JJ WRB': 977,\n",
       " 'VBG JJ NN': 3046,\n",
       " 'RB JJ DT': 2119,\n",
       " 'VBD RP VBG': 2951,\n",
       " 'NN NN FW': 1226,\n",
       " 'JJ TO PRP': 895,\n",
       " 'IN JJS PRP': 565,\n",
       " 'VBG VB': 3140,\n",
       " 'VBD PRP CC': 2903,\n",
       " 'VBP IN TO': 3361,\n",
       " 'NN CC': 1111,\n",
       " 'NN JJ CC': 1189,\n",
       " 'JJ RB VBG': 884,\n",
       " 'MD': 1063,\n",
       " 'NNS NNS VBG': 1616,\n",
       " 'VBD VBN DT': 2989,\n",
       " 'VBG RP TO': 3132,\n",
       " 'VBP DT VBZ': 3344,\n",
       " 'VB PRP NN': 2662,\n",
       " 'RB NN TO': 2153,\n",
       " 'NN JJ NNS': 1196,\n",
       " 'VBZ JJ CC': 3637,\n",
       " 'NN WRB NN': 1499,\n",
       " 'WRB RB VB': 4037,\n",
       " 'RB VBP NN': 2288,\n",
       " 'RB VBD VBG': 2251,\n",
       " 'JJ RP NN': 891,\n",
       " 'VB WP RB': 2785,\n",
       " 'RB PRP VB': 2182,\n",
       " 'VBZ VBP JJ': 3784,\n",
       " 'DT CD NN': 238,\n",
       " 'RB VBN IN': 2269,\n",
       " 'VBG CC RB': 3014,\n",
       " 'IN NNS VBZ': 613,\n",
       " 'VB PRP JJ': 2658,\n",
       " 'NN NNS TO': 1272,\n",
       " 'VBP PRP CC': 3440,\n",
       " 'NNS VBP JJ': 1725,\n",
       " 'RB VBZ VBN': 2315,\n",
       " 'JJ FW PRP': 761,\n",
       " 'DT VBP WP': 418,\n",
       " 'DT PRP RB': 343,\n",
       " 'PRP VBG IN': 1991,\n",
       " 'IN WP': 717,\n",
       " 'PRP NNS PRP': 1889,\n",
       " 'NN RBR JJ': 1327,\n",
       " 'PDT DT': 1772,\n",
       " 'PRP RB RBR': 1927,\n",
       " 'WDT VBP': 3847,\n",
       " 'VBZ VBP NNS': 3786,\n",
       " 'VBD NN VBG': 2877,\n",
       " 'NN IN IN': 1168,\n",
       " 'VBP VB DT': 3502,\n",
       " 'VB DT JJS': 2555,\n",
       " 'IN VBP PRP': 704,\n",
       " 'DT JJ JJ': 258,\n",
       " 'VBZ TO VB': 3741,\n",
       " 'VBN PRP': 3236,\n",
       " 'VB VBZ VB': 2778,\n",
       " 'NNS CC VBP': 1541,\n",
       " 'IN NN NN': 580,\n",
       " 'VBP WP NN': 3583,\n",
       " 'CC VB': 109,\n",
       " 'NN NNS VBG': 1275,\n",
       " 'IN VBN IN': 692,\n",
       " 'CC UH': 108,\n",
       " 'WP VBP DT': 3938,\n",
       " 'NN TO VB': 1350,\n",
       " 'IN JJ WRB': 555,\n",
       " 'DT VBP IN': 408,\n",
       " 'NNS VB': 1669,\n",
       " 'RB VBP VBD': 2295,\n",
       " 'WP VBZ VB': 3961,\n",
       " 'NNS VBG NN': 1703,\n",
       " 'CC PDT': 73,\n",
       " 'JJR IN PRP': 999,\n",
       " 'VBZ PRP NNS': 3694,\n",
       " 'VBD JJ VBD': 2855,\n",
       " 'PRP VBP RP': 2026,\n",
       " 'NN VBD VBG': 1389,\n",
       " 'RP IN DT': 2377,\n",
       " 'CC MD': 41,\n",
       " 'RB VBZ RB': 2309,\n",
       " 'TO TO': 2496,\n",
       " 'VBN RB PRP': 3254,\n",
       " 'VB DT VBG': 2566,\n",
       " 'IN DT VBN': 512,\n",
       " 'NNS PRP VBD': 1632,\n",
       " 'VBD TO VB': 2960,\n",
       " 'TO NN NN': 2475,\n",
       " 'VBP VB PRP': 3507,\n",
       " 'NN CC PRP': 1121,\n",
       " 'JJ CC': 731,\n",
       " 'JJ RB NN': 876,\n",
       " 'JJ VBD RB': 922,\n",
       " 'VBP DT NNS': 3334,\n",
       " 'NN CC RB': 1122,\n",
       " 'JJS NN NNS': 1046,\n",
       " 'VBZ RB RB': 3713,\n",
       " 'NNS RB CC': 1636,\n",
       " 'NNS VBG': 1698,\n",
       " 'RB TO': 2213,\n",
       " 'VBP IN PDT': 3358,\n",
       " 'WRB PDT DT': 4017,\n",
       " 'VBP WP VBP': 3587,\n",
       " 'VBP RB VBG': 3471,\n",
       " 'IN VBP DT': 700,\n",
       " 'VB VB': 2713,\n",
       " 'JJS NN WDT': 1052,\n",
       " 'NN NN EX': 1225,\n",
       " 'MD PRP NN': 1079,\n",
       " 'RP JJ PRP': 2391,\n",
       " 'JJ NNS VBP': 851,\n",
       " 'VBD NNS IN': 2888,\n",
       " 'VBP JJ IN': 3374,\n",
       " 'RB IN VBG': 2112,\n",
       " 'DT PRP': 338,\n",
       " 'PRP JJS': 1851,\n",
       " 'IN NNS WDT': 614,\n",
       " 'MD IN NN': 1071,\n",
       " 'NNS NN VBG': 1601,\n",
       " 'VBG NN JJ': 3064,\n",
       " 'WP JJ JJ': 3887,\n",
       " 'VBZ IN TO': 3633,\n",
       " 'VBD NN JJ': 2867,\n",
       " 'CC RB VBP': 104,\n",
       " 'JJ NN VB': 823,\n",
       " 'VBP PRP TO': 3451,\n",
       " 'RB VBP DT': 2284,\n",
       " 'IN JJ VBP': 553,\n",
       " 'CC PRP VB': 85,\n",
       " 'JJ JJ WRB': 793,\n",
       " 'VBG PRP': 3095,\n",
       " 'RB NNS RB': 2166,\n",
       " 'DT RBR JJ': 363,\n",
       " 'VBP CC VB': 3318,\n",
       " 'DT VBD': 382,\n",
       " 'WRB PRP JJ': 4022,\n",
       " 'CC NN PRP': 52,\n",
       " 'RB VBP CD': 2283,\n",
       " 'VBG IN VBG': 3038,\n",
       " 'WP DT': 3878,\n",
       " 'PRP VBP CD': 2015,\n",
       " 'WP NN IN': 3895,\n",
       " 'NN NNS VBD': 1274,\n",
       " 'JJ IN VBP': 774,\n",
       " 'IN NN RB': 583,\n",
       " 'PRP VBZ TO': 2047,\n",
       " 'TO VB DT': 2500,\n",
       " 'RP JJ VB': 2392,\n",
       " 'VBZ RB NN': 3710,\n",
       " 'VB VB WRB': 2730,\n",
       " 'VB VB RP': 2723,\n",
       " 'IN FW PRP': 525,\n",
       " 'NNS RB VBN': 1649,\n",
       " 'IN VBG RB': 686,\n",
       " 'NNS NN VBP': 1602,\n",
       " 'NN VBN PRP': 1419,\n",
       " 'VBG WP': 3164,\n",
       " 'VBZ NN PRP': 3666,\n",
       " 'VBD CC RB': 2807,\n",
       " 'NN JJ DT': 1190,\n",
       " 'IN JJ NN': 543,\n",
       " 'VBZ RB VBP': 3719,\n",
       " 'VBP PRP VB': 3452,\n",
       " 'NNS WP VBP': 1764,\n",
       " 'NNS RP': 1659,\n",
       " 'NN PRP CC': 1285,\n",
       " 'NN WDT': 1473,\n",
       " 'CC VB JJ': 113,\n",
       " 'WP VBD NNS': 3930,\n",
       " 'VBZ RBS': 3725,\n",
       " 'NN NNP PRP': 1256,\n",
       " 'VBN CC RB': 3174,\n",
       " 'IN JJ VBZ': 554,\n",
       " 'RBR VB': 2352,\n",
       " 'CC JJ VBZ': 38,\n",
       " 'RP RB RB': 2425,\n",
       " 'VB NN DT': 2618,\n",
       " 'NN RB RB': 1314,\n",
       " 'IN IN CD': 527,\n",
       " 'JJ JJ VBG': 789,\n",
       " 'DT JJ DT': 255,\n",
       " 'WP VBD PRP': 3931,\n",
       " 'NNS VB PRP': 1675,\n",
       " 'IN RB IN': 646,\n",
       " 'VBP VBD DT': 3519,\n",
       " 'DT CC VBD': 234,\n",
       " 'NN NN UH': 1242,\n",
       " 'NNP VBZ JJ': 1524,\n",
       " 'PRP PRP VBD': 1913,\n",
       " 'DT VBZ PRP': 427,\n",
       " 'IN DT VBG': 511,\n",
       " 'JJ VBP CC': 945,\n",
       " 'WDT VBD': 3835,\n",
       " 'VBD NN MD': 2868,\n",
       " 'IN VBN NN': 694,\n",
       " 'VBN MD VB': 3212,\n",
       " 'DT JJ IN': 257,\n",
       " 'VBP CC JJ': 3315,\n",
       " 'JJ IN DT': 763,\n",
       " 'JJ JJ CC': 778,\n",
       " 'DT VBP NNS': 411,\n",
       " 'VBD RB VB': 2933,\n",
       " 'VBG CC': 3011,\n",
       " 'CD NNS': 211,\n",
       " 'VB VBG RB': 2746,\n",
       " 'EX VBP RB': 457,\n",
       " 'NN NN WDT': 1249,\n",
       " 'JJ NNS NNS': 841,\n",
       " 'JJ RB VBN': 885,\n",
       " 'VB PRP WRB': 2676,\n",
       " 'NN PRP DT': 1286,\n",
       " 'PRP JJR VBD': 1850,\n",
       " 'VB VBG NNS': 2744,\n",
       " 'IN VB DT': 668,\n",
       " 'DT NNS VB': 329,\n",
       " 'VBP VBP': 3555,\n",
       " 'NNS NN NN': 1594,\n",
       " 'NNS NN IN': 1591,\n",
       " 'IN PRP CD': 621,\n",
       " 'TO PDT DT': 2483,\n",
       " 'NN VBD WRB': 1394,\n",
       " 'WRB JJ VBD': 3987,\n",
       " 'VBD NNS MD': 2890,\n",
       " 'NNS JJ VBZ': 1581,\n",
       " 'TO PRP NNS': 2489,\n",
       " 'PRP MD VB': 1858,\n",
       " 'PRP CC DT': 1779,\n",
       " 'MD VB RP': 1102,\n",
       " 'JJS JJ NNS': 1039,\n",
       " 'JJ VBD DT': 916,\n",
       " 'NN RB': 1304,\n",
       " 'PRP VBP PDT': 2023,\n",
       " 'RP NN RB': 2402,\n",
       " 'JJ FW': 758,\n",
       " 'RB CC RB': 2076,\n",
       " 'NN VBD VBP': 1391,\n",
       " 'NNS VBD RP': 1693,\n",
       " 'JJ VBP JJ': 950,\n",
       " 'VBN IN': 3186,\n",
       " 'VBZ JJ JJ': 3640,\n",
       " 'VB JJR JJ': 2608,\n",
       " 'VBP VBN RP': 3549,\n",
       " 'NNS NNS': 1605,\n",
       " 'WP VBG': 3936,\n",
       " 'NNS DT VBP': 1551,\n",
       " 'NNS VBG PRP': 1705,\n",
       " 'VB NN VBD': 2628,\n",
       " 'VB WRB MD': 2794,\n",
       " 'VBN VBN NN': 3299,\n",
       " 'RB IN': 2101,\n",
       " 'JJR RB': 1018,\n",
       " 'JJS DT': 1032,\n",
       " 'CC RB VBN': 103,\n",
       " 'RB RB VBN': 2203,\n",
       " 'TO RB': 2494,\n",
       " 'DT VBP NN': 410,\n",
       " 'CC RB JJ': 94,\n",
       " 'NNS WRB NN': 1769,\n",
       " 'VB NN WP': 2634,\n",
       " 'TO VB JJR': 2503,\n",
       " 'VBP DT JJS': 3332,\n",
       " 'WP RB VBZ': 3915,\n",
       " 'PRP DT JJ': 1796,\n",
       " 'NN WRB VB': 1503,\n",
       " 'NNS RBR': 1653,\n",
       " 'WP NN NN': 3897,\n",
       " 'NN NN TO': 1241,\n",
       " 'NN VB VBG': 1366,\n",
       " 'VB VB VBN': 2727,\n",
       " 'WDT VBP VBN': 3858,\n",
       " 'NN TO VBG': 1351,\n",
       " 'DT NN VBZ': 311,\n",
       " 'VBP RP NNS': 3484,\n",
       " 'DT DT JJ': 242,\n",
       " 'RB IN IN': 2104,\n",
       " 'NN VBG VBD': 1408,\n",
       " 'NN NN RBS': 1239,\n",
       " 'NN PDT DT': 1283,\n",
       " 'RBR NNS IN': 2345,\n",
       " 'RP PRP JJ': 2414,\n",
       " 'NN JJS NN': 1212,\n",
       " 'CC RB IN': 93,\n",
       " 'MD RB RB': 1087,\n",
       " 'NNS VBG CC': 1699,\n",
       " 'MD DT': 1066,\n",
       " 'IN DT JJS': 502,\n",
       " 'NNS RB': 1635,\n",
       " 'WRB JJ VBG': 3988,\n",
       " 'PRP PRP NN': 1909,\n",
       " 'VBP IN WRB': 3368,\n",
       " 'JJ NNS VB': 847,\n",
       " 'JJ VBD': 913,\n",
       " 'VBD NN TO': 2874,\n",
       " 'NN NNS DT': 1262,\n",
       " 'PRP NN RBR': 1869,\n",
       " 'RB PRP IN': 2176,\n",
       " 'JJ NN JJ': 810,\n",
       " 'NN IN JJS': 1171,\n",
       " 'IN NN DT': 573,\n",
       " 'NN WP NN': 1489,\n",
       " 'VB JJ VBP': 2603,\n",
       " 'VBG TO VB': 3139,\n",
       " 'IN NNS PRP': 605,\n",
       " 'VBG NNS VBP': 3090,\n",
       " 'DT NN WP': 313,\n",
       " 'IN CC JJ': 484,\n",
       " 'WP VB JJ': 3920,\n",
       " 'UH PRP': 2532,\n",
       " 'RP RB VBN': 2428,\n",
       " 'TO PRP': 2484,\n",
       " 'IN DT CC': 497,\n",
       " 'RB VB CD': 2221,\n",
       " 'IN FW DT': 522,\n",
       " 'IN FW': 521,\n",
       " 'IN NNS VBD': 609,\n",
       " 'JJ VBD CD': 915,\n",
       " 'RB VBG DT': 2255,\n",
       " 'VB VBN RB': 2759,\n",
       " 'VB DT CC': 2549,\n",
       " 'PRP CD JJ': 1791,\n",
       " 'VBP PRP DT': 3441,\n",
       " 'VBD PRP VB': 2916,\n",
       " 'WDT VBZ VBN': 3873,\n",
       " 'NN WRB JJ': 1497,\n",
       " 'VBD TO IN': 2954,\n",
       " 'CC JJ DT': 25,\n",
       " 'VBD RB JJ': 2926,\n",
       " 'NN VBD JJR': 1377,\n",
       " 'VBN TO VB': 3276,\n",
       " 'VBZ VBG RB': 3763,\n",
       " 'WP TO VB': 3917,\n",
       " 'RB NN NNS': 2149,\n",
       " 'NN VBD DT': 1374,\n",
       " 'VBN JJ RB': 3207,\n",
       " 'RB VBD CC': 2240,\n",
       " 'WP VBZ JJ': 3953,\n",
       " 'NN VBG WRB': 1411,\n",
       " 'WRB TO': 4039,\n",
       " 'VBP PRP': 3439,\n",
       " 'WP JJ VBP': 3891,\n",
       " 'WP RB VBD': 3913,\n",
       " 'IN DT NNS': 505,\n",
       " 'IN PRP VB': 634,\n",
       " 'JJS VB DT': 1062,\n",
       " 'VBG JJR NN': 3058,\n",
       " 'NNS VBD': 1684,\n",
       " 'VB WP JJ': 2782,\n",
       " 'VBP PDT DT': 3438,\n",
       " 'PRP VBZ PRP': 2044,\n",
       " 'PRP PDT': 1901,\n",
       " 'NNS IN IN': 1558,\n",
       " 'JJR VB': 1025,\n",
       " 'PRP FW': 1808,\n",
       " 'JJS TO': 1059,\n",
       " 'JJ JJR NN': 795,\n",
       " 'VBN VBZ': 3307,\n",
       " 'IN DT WDT': 515,\n",
       " 'FW RB': 479,\n",
       " 'VBP NNS TO': 3428,\n",
       " 'WDT VBP JJ': 3850,\n",
       " 'VBZ VBZ DT': 3791,\n",
       " 'VBP NNS IN': 3421,\n",
       " 'NNS NN NNS': 1595,\n",
       " 'VBD WRB DT': 3005,\n",
       " 'VBP RP CC': 3479,\n",
       " 'RB JJ IN': 2120,\n",
       " 'JJ VBP VBG': 959,\n",
       " 'NNS IN PRP': 1563,\n",
       " 'WP MD VB': 3893,\n",
       " 'TO VB VBD': 2512,\n",
       " 'VBG PRP VBZ': 3111,\n",
       " 'VBN TO PRP': 3275,\n",
       " 'CC VBP VB': 161,\n",
       " 'NNP JJ': 1509,\n",
       " 'MD VB JJR': 1097,\n",
       " 'WRB VBZ NN': 4059,\n",
       " 'JJ DT JJ': 752,\n",
       " 'NN VBN RB': 1420,\n",
       " 'VBZ WP VBZ': 3802,\n",
       " 'JJ VB NNS': 904,\n",
       " 'NN VBP PDT': 1437,\n",
       " 'JJS IN PRP': 1036,\n",
       " 'JJ VBZ IN': 968,\n",
       " 'FW DT JJ': 463,\n",
       " 'RB VB RB': 2229,\n",
       " 'VBP WP VBD': 3586,\n",
       " 'CC DT JJ': 7,\n",
       " 'CC MD RB': 42,\n",
       " 'RB VBP JJ': 2286,\n",
       " 'NNS VBN NNS': 1715,\n",
       " 'JJ PRP IN': 860,\n",
       " 'IN VBD DT': 674,\n",
       " 'VBZ RB VBN': 3718,\n",
       " 'VBZ NN CC': 3659,\n",
       " 'CC JJ VBN': 36,\n",
       " 'NN WP MD': 1488,\n",
       " 'VBZ IN PRP': 3631,\n",
       " 'NNS NN RB': 1597,\n",
       " 'VBN RB VBD': 3258,\n",
       " 'RB NN DT': 2144,\n",
       " 'CD JJ NN': 192,\n",
       " 'JJ NN PRP': 818,\n",
       " 'JJ JJ VBD': 788,\n",
       " 'IN EX VBD': 519,\n",
       " 'JJR NN IN': 1007,\n",
       " 'PRP WDT': 2056,\n",
       " 'VBP VBG VBP': 3539,\n",
       " 'JJR TO VB': 1024,\n",
       " 'PRP FW PRP': 1809,\n",
       " 'VBD PRP WRB': 2921,\n",
       " 'VBZ RB DT': 3707,\n",
       " 'VBN NN VBD': 3224,\n",
       " 'DT RB NN': 352,\n",
       " 'NNS VBN': 1711,\n",
       " 'DT RBR NN': 364,\n",
       " 'VBZ PRP': 3687,\n",
       " 'IN WP PRP': 721,\n",
       " 'WP VBZ NNS': 3956,\n",
       " 'UH VB PRP': 2535,\n",
       " 'RP VBN': 2444,\n",
       " 'CC JJ IN': 26,\n",
       " 'NN WP VBP': 1493,\n",
       " 'VB JJ VBZ': 2604,\n",
       " 'WP PRP VBZ': 3910,\n",
       " 'JJ DT NNS': 754,\n",
       " 'VB VB WP': 2729,\n",
       " 'VBG RP JJ': 3127,\n",
       " 'VBN VBG NN': 3292,\n",
       " 'VBN': 3169,\n",
       " 'CC VBD NN': 133,\n",
       " 'JJS NN TO': 1049,\n",
       " 'PRP NN PRP': 1867,\n",
       " 'JJ VBD RP': 923,\n",
       " 'PRP VBG VBN': 2000,\n",
       " 'VBD CC DT': 2803,\n",
       " 'NN NN JJS': 1230,\n",
       " 'NN CD': 1132,\n",
       " 'NN VBN VB': 1423,\n",
       " 'NNP NNP': 1517,\n",
       " 'IN NNS DT': 599,\n",
       " 'PRP NNS CC': 1882,\n",
       " 'JJ JJ NN': 782,\n",
       " 'NN CC VB': 1124,\n",
       " 'VBP DT NN': 3333,\n",
       " 'VB PRP RB': 2666,\n",
       " 'WP NN VBD': 3899,\n",
       " 'RB DT NNS': 2093,\n",
       " 'IN VBP': 699,\n",
       " 'JJ CC NN': 735,\n",
       " 'TO VB VBG': 2513,\n",
       " 'CC NNS MD': 66,\n",
       " 'PRP VBZ DT': 2038,\n",
       " 'IN RP PRP': 660,\n",
       " 'VBD JJ IN': 2847,\n",
       " 'CD NN CC': 197,\n",
       " 'VBG VBN CC': 3155,\n",
       " 'NNS IN WRB': 1569,\n",
       " 'NN NN VBG': 1245,\n",
       " 'RB RB VBG': 2202,\n",
       " 'PRP TO PRP': 1950,\n",
       " 'JJR DT NN': 992,\n",
       " 'PRP VBD DT': 1973,\n",
       " 'PRP NN WP': 1879,\n",
       " 'VBD TO': 2952,\n",
       " 'PRP PRP': 1903,\n",
       " 'IN JJS JJ': 562,\n",
       " 'VBD PRP VBZ': 2920,\n",
       " 'JJS NN CC': 1042,\n",
       " 'DT WRB': 440,\n",
       " 'RB NN WRB': 2160,\n",
       " 'IN VBG IN': 680,\n",
       " 'CC VBZ RP': 171,\n",
       " 'RB WP': 2318,\n",
       " 'VBD PRP TO': 2915,\n",
       " 'DT PRP VBZ': 346,\n",
       " 'JJS JJ VBP': 1040,\n",
       " 'DT JJ CC': 253,\n",
       " 'IN JJ MD': 542,\n",
       " 'CC RB RB': 99,\n",
       " 'VBN RP': 3261,\n",
       " 'RB VB VB': 2233,\n",
       " 'PRP NNS VBP': 1897,\n",
       " 'RB IN VBZ': 2114,\n",
       " 'NN NNS RB': 1269,\n",
       " 'RP VB PRP': 2436,\n",
       " 'IN JJ CC': 537,\n",
       " 'WRB NN VBD': 4005,\n",
       " 'NN FW FW': 1160,\n",
       " 'VBN DT': 3180,\n",
       " 'NN DT VBG': 1149,\n",
       " 'VBP VBN VBG': 3551,\n",
       " 'DT VBZ NN': 425,\n",
       " 'TO JJ NN': 2468,\n",
       " 'VBP VB WP': 3516,\n",
       " 'NNS VB VB': 1679,\n",
       " 'NN DT DT': 1139,\n",
       " 'TO PDT': 2482,\n",
       " 'WP VBZ VBP': 3964,\n",
       " 'RP NN JJ': 2398,\n",
       " 'NNS NNS CC': 1606,\n",
       " 'RB PRP JJ': 2177,\n",
       " 'VB IN VBG': 2584,\n",
       " 'PDT IN': 1777,\n",
       " 'PRP IN CC': 1811,\n",
       " 'CD PRP': 219,\n",
       " 'DT VBP': 406,\n",
       " 'IN VBZ WP': 714,\n",
       " 'VBN PRP VBZ': 3247,\n",
       " 'NN IN TO': 1179,\n",
       " 'VBD IN CC': 2829,\n",
       " 'CD NNS VBD': 217,\n",
       " 'VBP NN DT': 3401,\n",
       " 'WRB VB DT': 4042,\n",
       " 'PRP VBP JJR': 2019,\n",
       " 'RB RB DT': 2190,\n",
       " 'VB VBP IN': 2766,\n",
       " 'VBD NN IN': 2866,\n",
       " 'JJ VBN TO': 943,\n",
       " 'JJR JJ NN': 1004,\n",
       " 'NNS VB WRB': 1683,\n",
       " 'VBN RP NN': 3266,\n",
       " 'VB TO PRP': 2711,\n",
       " 'DT VBZ NNS': 426,\n",
       " 'VBZ WDT': 3795,\n",
       " 'VB IN TO': 2582,\n",
       " 'PRP RP CC': 1937,\n",
       " 'PDT': 1771,\n",
       " 'IN JJR JJ': 558,\n",
       " 'VBN VBD': 3283,\n",
       " 'PRP RB VBN': 1932,\n",
       " 'VB RB VBP': 2692,\n",
       " 'EX RB': 450,\n",
       " 'VB NNS PRP': 2643,\n",
       " 'VBG JJ DT': 3043,\n",
       " 'DT RB CC': 348,\n",
       " 'VBP PRP NNS': 3447,\n",
       " 'VBN CC VBN': 3176,\n",
       " 'PRP NNS RB': 1890,\n",
       " 'DT NN FW': 292,\n",
       " 'NNS NNS VBP': 1617,\n",
       " 'IN RBR': 658,\n",
       " 'VBD VBG NN': 2981,\n",
       " 'NN VB DT': 1356,\n",
       " 'VBZ VBD IN': 3752,\n",
       " 'VBG PRP NNS': 3102,\n",
       " 'PRP IN NN': 1817,\n",
       " 'PRP VBP WP': 2034,\n",
       " 'TO DT IN': 2458,\n",
       " 'WDT VBZ PRP': 3867,\n",
       " 'VBZ VBZ PRP': 3793,\n",
       " 'PRP JJR IN': 1846,\n",
       " 'VBP JJ VBN': 3386,\n",
       " 'NNS DT JJ': 1548,\n",
       " 'VBN RB NNS': 3253,\n",
       " 'RB JJ PRP': 2126,\n",
       " 'IN NNS VBG': 610,\n",
       " 'CC DT VBZ': 14,\n",
       " 'VBD RBR': 2938,\n",
       " 'VBZ RP DT': 3729,\n",
       " 'NN VB NN': 1359,\n",
       " 'WRB JJ VBP': 3990,\n",
       " 'VBZ JJ NNS': 3642,\n",
       " 'TO PRP CC': 2485,\n",
       " 'VBP JJR JJ': 3392,\n",
       " 'NN VBZ RBS': 1463,\n",
       " 'JJ VB VB': 908,\n",
       " 'VBG PRP JJ': 3099,\n",
       " 'RP IN IN': 2378,\n",
       " 'CC VBP NN': 156,\n",
       " 'VBP NNS DT': 3420,\n",
       " 'MD TO VB': 1090,\n",
       " 'PRP MD IN': 1856,\n",
       " 'VBP VB JJ': 3504,\n",
       " 'VBP PRP VBZ': 3457,\n",
       " 'VBN MD': 3211,\n",
       " 'VB JJR RB': 2611,\n",
       " 'VB VBN RP': 2760,\n",
       " 'NNS VBZ DT': 1744,\n",
       " 'TO VB IN': 2501,\n",
       " 'VBP VB': 3499,\n",
       " 'VBZ VBN RP': 3776,\n",
       " 'PRP RP NNS': 1942,\n",
       " 'VBD PRP RB': 2913,\n",
       " 'VBG NNS NN': 3083,\n",
       " 'NNS PRP RB': 1629,\n",
       " 'IN RB RB': 651,\n",
       " 'DT CD NNS': 239,\n",
       " 'VBP VBG NN': 3530,\n",
       " 'WP NNS VBP': 3903,\n",
       " 'DT NNS JJ': 320,\n",
       " 'DT JJ RB': 264,\n",
       " 'PRP NN CC': 1860,\n",
       " 'VBD NNS': 2886,\n",
       " 'JJ VBP IN': 949,\n",
       " 'NN VBZ PRP': 1460,\n",
       " 'VB WP PRP': 2784,\n",
       " 'VBD PRP VBG': 2918,\n",
       " 'JJS NNS': 1053,\n",
       " 'PRP VBP WRB': 2035,\n",
       " 'CC VBP PRP': 158,\n",
       " 'EX DT': 442,\n",
       " 'VBG JJ PRP': 3048,\n",
       " 'CC VB NNS': 117,\n",
       " 'VBN VBP IN': 3303,\n",
       " 'VBN NN VB': 3223,\n",
       " 'NN JJ IN': 1191,\n",
       " 'IN VB JJ': 669,\n",
       " 'CC VBG DT': 141,\n",
       " 'VBG NNS IN': 3081,\n",
       " 'JJ IN TO': 772,\n",
       " 'VBP VBZ': 3569,\n",
       " 'JJ VBZ RB': 972,\n",
       " 'NN VBG VBG': 1409,\n",
       " 'VBN RP CC': 3262,\n",
       " 'CC VB DT': 111,\n",
       " 'IN NNS RB': 606,\n",
       " 'NN WRB DT': 1496,\n",
       " 'VBZ VB NN': 3746,\n",
       " 'WRB NNS VB': 4013,\n",
       " 'VBP VBZ RB': 3575,\n",
       " 'JJ VBN IN': 938,\n",
       " 'NN VBD IN': 1375,\n",
       " 'VBZ CC NN': 3600,\n",
       " 'WP DT NNS': 3881,\n",
       " 'DT NN WDT': 312,\n",
       " 'VBN CD NNS': 3179,\n",
       " 'RB RBR JJ': 2210,\n",
       " 'JJ WRB NNS': 982,\n",
       " 'WRB JJ VBN': 3989,\n",
       " 'NNS RBR VB': 1657,\n",
       " 'VBP PRP RB': 3449,\n",
       " 'JJ JJR VB': 796,\n",
       " 'RB DT IN': 2089,\n",
       " 'RB VB NN': 2226,\n",
       " 'JJS PRP': 1057,\n",
       " 'RBR VB DT': 2353,\n",
       " 'PRP WRB NN': 2064,\n",
       " 'RB JJ': 2116,\n",
       " 'RB CC VBG': 2079,\n",
       " 'VBZ TO DT': 3738,\n",
       " 'JJS NN PRP': 1047,\n",
       " 'VBD VB NN': 2965,\n",
       " 'NNP JJ NN': 1510,\n",
       " 'VBP WP IN': 3581,\n",
       " 'CC VBN': 149,\n",
       " 'VBD DT': 2815,\n",
       " 'VBD PRP RP': 2914,\n",
       " 'NN JJ JJ': 1192,\n",
       " 'VBZ EX': 3621,\n",
       " 'VBZ VBD DT': 3751,\n",
       " 'VBP VBG RP': 3534,\n",
       " 'PDT DT NN': 1774,\n",
       " 'NN WDT CD': 1474,\n",
       " 'PRP VB DT': 1954,\n",
       " 'WDT VBD TO': 3844,\n",
       " 'VB IN VB': 2583,\n",
       " 'WP PRP JJ': 3905,\n",
       " 'WRB JJ NN': 3982,\n",
       " 'JJR NNS VBP': 1015,\n",
       " 'DT RBS NNS': 368,\n",
       " 'VBZ VBG IN': 3758,\n",
       " 'TO DT': 2457,\n",
       " 'VBP DT CD': 3327,\n",
       " 'IN NNS IN': 600,\n",
       " 'WDT VBD PRP': 3841,\n",
       " 'VBD DT NN': 2821,\n",
       " 'PRP VBD VBG': 1985,\n",
       " 'VBP DT VBN': 3342,\n",
       " 'VB JJ': 2587,\n",
       " 'VBD RP JJ': 2944,\n",
       " 'VBD RB VBP': 2937,\n",
       " 'VBD IN PRP': 2837,\n",
       " 'TO DT JJ': 2459,\n",
       " 'RB RB WP': 2206,\n",
       " 'TO NNS': 2479,\n",
       " 'RBS RB': 2364,\n",
       " 'RB VBN VB': 2277,\n",
       " 'VB PRP NNS': 2663,\n",
       " 'JJ IN JJS': 766,\n",
       " 'PRP VBZ NNS': 2043,\n",
       " 'DT NNS IN': 319,\n",
       " 'VB TO NN': 2710,\n",
       " 'DT JJ': 252,\n",
       " 'VBG WRB': 3166,\n",
       " 'VBN JJ VBP': 3209,\n",
       " 'VBG NN VBG': 3073,\n",
       " 'PRP VBG DT': 1990,\n",
       " 'DT VBZ RP': 429,\n",
       " 'MD VB CC': 1092,\n",
       " 'RBR JJ': 2338,\n",
       " 'VBN RP NNS': 3267,\n",
       " 'NNS VBD VBG': 1695,\n",
       " 'MD RB VB': 1088,\n",
       " 'PRP VBP PRP': 2024,\n",
       " 'CD JJ': 191,\n",
       " 'WRB NN JJ': 3998,\n",
       " 'WP VBD IN': 3927,\n",
       " 'NN FW': 1159,\n",
       " 'PRP NNS NN': 1887,\n",
       " 'VB JJR NNS': 2610,\n",
       " 'RBR IN DT': 2333,\n",
       " 'PRP JJ WRB': 1843,\n",
       " 'RB JJR IN': 2138,\n",
       " 'NN NN IN': 1227,\n",
       " 'VBG VBD': 3145,\n",
       " 'VB DT PRP': 2558,\n",
       " 'JJ IN RB': 771,\n",
       " 'PRP PRP CC': 1904,\n",
       " 'IN TO DT': 662,\n",
       " 'VBD VBG IN': 2979,\n",
       " 'PRP NN VB': 1872,\n",
       " 'IN NN VB': 586,\n",
       " 'JJ NN VBG': 825,\n",
       " 'EX MD VB': 447,\n",
       " 'CC VB VBG': 124,\n",
       " 'JJ NNS IN': 837,\n",
       " 'VBZ VBN': 3767,\n",
       " 'VBN NN IN': 3216,\n",
       " 'NNS PRP TO': 1630,\n",
       " 'NN VBZ CD': 1452,\n",
       " 'DT JJ FW': 256,\n",
       " 'RB NN VBD': 2155,\n",
       " 'IN CC DT': 482,\n",
       " 'VBD DT PRP': 2823,\n",
       " 'PRP VBP RB': 2025,\n",
       " 'NN DT': 1137,\n",
       " 'VBZ VBP PRP': 3787,\n",
       " 'NNS TO': 1664,\n",
       " 'NN WP RB': 1491,\n",
       " 'NN VBZ DT': 1453,\n",
       " 'VBZ VBN RB': 3775,\n",
       " 'VBZ VB': 3742,\n",
       " 'NNS VBZ VBP': 1752,\n",
       " 'RB NN VBZ': 2158,\n",
       " 'NN VBD RB': 1383,\n",
       " 'VBP PRP WRB': 3458,\n",
       " 'NNS VBG VBD': 1709,\n",
       " 'VBD VBD DT': 2973,\n",
       " 'FW PRP NN': 478,\n",
       " 'RB CC IN': 2071,\n",
       " 'VBZ RB CC': 3706,\n",
       " 'RB VBG PRP': 2260,\n",
       " 'VBP NN RB': 3408,\n",
       " 'NNS RBR JJ': 1655,\n",
       " 'VBP RB IN': 3462,\n",
       " 'DT VBG NNS': 399,\n",
       " 'RB JJ RB': 2127,\n",
       " 'TO NN PRP': 2476,\n",
       " 'IN VBD PRP': 676,\n",
       " 'RP RB VBG': 2427,\n",
       " 'VBP NN MD': 3404,\n",
       " 'NN VB TO': 1364,\n",
       " 'CC NN WDT': 60,\n",
       " 'NN VBG CC': 1396,\n",
       " 'JJ NNS WDT': 853,\n",
       " 'PRP VB VBG': 1965,\n",
       " 'RB NNS VBZ': 2170,\n",
       " 'VBZ RB WRB': 3721,\n",
       " 'NN TO RB': 1349,\n",
       " 'NN IN JJ': 1169,\n",
       " 'VBZ RP': 3727,\n",
       " 'NN VB CD': 1355,\n",
       " 'NN VBP WP': 1448,\n",
       " 'VB DT VBD': 2565,\n",
       " 'VBG RP RB': 3131,\n",
       " 'NN CC MD': 1118,\n",
       " 'WP IN NN': 3885,\n",
       " 'NN NNS VBP': 1277,\n",
       " 'PRP IN RB': 1820,\n",
       " 'NNS DT': 1546,\n",
       " 'VB JJ NNP': 2594,\n",
       " 'RP PRP': 2413,\n",
       " 'DT VB VBN': 381,\n",
       " 'VBG RB RB': 3118,\n",
       " 'RP VBP': 2445,\n",
       " 'MD JJ': 1074,\n",
       " 'VBD TO NNS': 2957,\n",
       " 'MD VB': 1091,\n",
       " 'CD RB': 220,\n",
       " 'CC TO VB': 107,\n",
       " 'VBZ PRP JJ': 3691,\n",
       " 'VB IN RP': 2581,\n",
       " 'PRP VBZ WP': 2054,\n",
       " 'WDT VBZ DT': 3861,\n",
       " 'VBN RB IN': 3250,\n",
       " 'WDT': 3809,\n",
       " 'NN VBG DT': 1397,\n",
       " 'RBS NNS': 2363,\n",
       " 'VBP VBG TO': 3535,\n",
       " 'VBZ RP IN': 3730,\n",
       " 'VBP RB JJ': 3463,\n",
       " 'RB EX VB': 2100,\n",
       " 'VBP JJ RB': 3381,\n",
       " 'VBG PRP DT': 3097,\n",
       " 'VBZ PRP IN': 3690,\n",
       " 'JJ JJ RB': 785,\n",
       " 'VB IN CD': 2573,\n",
       " 'VBG IN PRP': 3035,\n",
       " 'WDT VBD IN': 3837,\n",
       " 'RB NN VB': 2154,\n",
       " 'NN VBP DT': 1431,\n",
       " 'PRP VBZ CC': 2037,\n",
       " 'PRP TO DT': 1949,\n",
       " 'VBP DT VBD': 3340,\n",
       " 'VBZ JJR NNS': 3654,\n",
       " 'VBN VB DT': 3278,\n",
       " 'IN NN CC': 571,\n",
       " 'NNS VBP WRB': 1742,\n",
       " 'VBZ NN DT': 3660,\n",
       " 'VBG NN': 3059,\n",
       " 'JJ DT IN': 751,\n",
       " 'VBP IN VBP': 3365,\n",
       " 'IN JJ NNS': 545,\n",
       " 'VBD RP CC': 2941,\n",
       " 'VBZ PRP WRB': 3704,\n",
       " 'VBN NN TO': 3222,\n",
       " 'VBD NN VBN': 2878,\n",
       " 'CC JJ NNS': 30,\n",
       " 'VBG NN WP': 3077,\n",
       " 'JJ NN VBN': 826,\n",
       " 'IN VBZ PRP': 711,\n",
       " 'EX VBP NNS': 456,\n",
       " 'RP RP': 2430,\n",
       " 'JJ MD': 800,\n",
       " 'VBG NNS TO': 3087,\n",
       " 'VBN JJ NNS': 3205,\n",
       " 'VBG JJ TO': 3050,\n",
       " 'DT CC NN': 231,\n",
       " 'VBZ JJR': 3651,\n",
       " 'RB CC VBP': 2080,\n",
       " 'CC VBZ DT': 166,\n",
       " 'IN RB NNS': 649,\n",
       " 'VB VBG': 2738,\n",
       " 'VBD WRB NN': 3007,\n",
       " 'CC NNS RB': 69,\n",
       " 'NN RB RBR': 1315,\n",
       " 'NN IN CD': 1164,\n",
       " 'VBP RB VB': 3469,\n",
       " 'VBG RB': 3113,\n",
       " 'CC VBG NN': 144,\n",
       " 'PRP VB VB': 1964,\n",
       " 'VBP WRB': 3589,\n",
       " 'VBD RP VB': 2950,\n",
       " 'VBP CC PRP': 3316,\n",
       " 'VB DT NNS': 2557,\n",
       " 'RB VBZ VBD': 2313,\n",
       " 'JJ VB IN': 901,\n",
       " 'NNS VBP VBP': 1739,\n",
       " 'PRP NN': 1859,\n",
       " 'WRB PRP VB': 4027,\n",
       " 'VBG PRP VBG': 3109,\n",
       " 'WP VBP PRP': 3943,\n",
       " 'UH': 2524,\n",
       " 'MD VB PRP': 1100,\n",
       " 'VBD WP PRP': 3003,\n",
       " 'VBD IN CD': 2830,\n",
       " 'VBN IN VBG': 3196,\n",
       " 'WRB PRP MD': 4023,\n",
       " 'NN VBN DT': 1414,\n",
       " 'JJ RB CC': 872,\n",
       " 'NNS JJ TO': 1578,\n",
       " 'IN PRP FW': 623,\n",
       " 'CD IN NN': 189,\n",
       " 'RB JJ WP': 2135,\n",
       " 'VBD VB DT': 2962,\n",
       " 'PRP PRP JJ': 1907,\n",
       " 'VB RB VBG': 2690,\n",
       " 'PDT DT NNS': 1775,\n",
       " 'NNS RB VBZ': 1651,\n",
       " 'VBG PRP IN': 3098,\n",
       " 'TO PRP RB': 2491,\n",
       " 'NNS VBP TO': 1734,\n",
       " 'PRP RBR': 1935,\n",
       " 'NNS VBZ JJ': 1746,\n",
       " 'NN JJ JJR': 1193,\n",
       " 'VBD VB PRP': 2966,\n",
       " 'DT WDT VBP': 437,\n",
       " 'VB VBZ': 2775,\n",
       " 'JJ WRB PRP': 983,\n",
       " 'PRP JJS JJ': 1852,\n",
       " 'CC PRP VBP': 89,\n",
       " 'VBP NN VBP': 3414,\n",
       " 'JJ PRP VBZ': 870,\n",
       " 'DT NN IN': 293,\n",
       " 'NN CC NNS': 1120,\n",
       " 'FW NNS': 475,\n",
       " 'VBN RB NN': 3252,\n",
       " 'JJR VBG': 1028,\n",
       " 'RB RB VB': 2200,\n",
       " 'VBN VBP': 3301,\n",
       " 'PRP JJ JJ': 1831,\n",
       " 'NNS NN VB': 1599,\n",
       " 'RB VBP PRP': 2290,\n",
       " 'VB VB TO': 2724,\n",
       " 'VB RB': 2677,\n",
       " 'VB NNS IN': 2640,\n",
       " 'PRP PRP VBZ': 1916,\n",
       " 'VBD JJ NNS': 2850,\n",
       " 'VBG DT NNS': 3024,\n",
       " 'RBR JJ IN': 2339,\n",
       " 'IN VBN TO': 697,\n",
       " 'WRB NN VB': 4004,\n",
       " 'RB RB RB': 2198,\n",
       " 'VBP VBN VBP': 3553,\n",
       " 'VBZ JJ IN': 3639,\n",
       " 'VBD VBN NN': 2992,\n",
       " 'RB JJ MD': 2123,\n",
       " 'JJ NN WP': 830,\n",
       " 'CC NN NNS': 51,\n",
       " 'VBZ RB VB': 3716,\n",
       " 'MD VB NN': 1098,\n",
       " 'CC NN VBP': 58,\n",
       " 'JJ RBR': 888,\n",
       " 'MD VB RB': 1101,\n",
       " 'RBR IN NN': 2335,\n",
       " 'WRB VBD PRP': 4045,\n",
       " 'DT FW': 245,\n",
       " 'RB JJ TO': 2128,\n",
       " 'RBR': 2329,\n",
       " 'PRP JJ CC': 1828,\n",
       " 'VBD VBP': 2999,\n",
       " 'NN RP CC': 1334,\n",
       " 'PRP VBZ VBN': 2051,\n",
       " 'VBD DT IN': 2818,\n",
       " 'NN CC NN': 1119,\n",
       " 'VBP JJ NN': 3377,\n",
       " 'DT NNS CC': 317,\n",
       " 'VBD RB VBN': 2936,\n",
       " 'RP TO': 2431,\n",
       " 'VBP VBN NN': 3545,\n",
       " 'JJ WP': 975,\n",
       " 'TO': 2452,\n",
       " 'RB NN NN': 2148,\n",
       " 'DT RB VBD': 357,\n",
       " 'RP CC VB': 2369,\n",
       " 'CC VBZ RB': 170,\n",
       " 'RP NN VBD': 2403,\n",
       " 'PRP VBD WP': 1987,\n",
       " 'NN VBD VB': 1387,\n",
       " 'EX VBD DT': 453,\n",
       " 'PRP NN TO': 1871,\n",
       " 'JJ PRP RB': 865,\n",
       " 'VBG RB JJ': 3116,\n",
       " 'IN VBG PRP': 685,\n",
       " 'NN EX VBP': 1157,\n",
       " 'NN VBP WRB': 1449,\n",
       " 'WDT RB VBD': 3830,\n",
       " 'VBN JJ CC': 3200,\n",
       " 'NNS NNS MD': 1608,\n",
       " 'VBP RB WRB': 3475,\n",
       " 'VBN VBG JJ': 3291,\n",
       " 'VBD VBN TO': 2997,\n",
       " 'TO VB RP': 2509,\n",
       " 'VB NN PRP': 2624,\n",
       " 'VB WP DT': 2780,\n",
       " 'DT VBG DT': 395,\n",
       " 'CC IN NN': 19,\n",
       " 'VBP IN NNS': 3357,\n",
       " 'NNS NNS TO': 1613,\n",
       " 'JJ VB': 899,\n",
       " 'JJ NN PDT': 817,\n",
       " 'NN RB VBD': 1318,\n",
       " 'VBD RB PRP': 2929,\n",
       " 'IN NN RP': 584,\n",
       " 'JJ CD': 744,\n",
       " 'VBP DT RBS': 3337,\n",
       " 'RB VBN RP': 2275,\n",
       " 'WDT VBZ VB': 3871,\n",
       " 'DT VBZ RB': 428,\n",
       " 'IN JJ TO': 548,\n",
       " 'PRP VBN NN': 2006,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 7086)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 4063)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 17)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.3 ,  79.94,  30.  , ...,   1.  ,   0.  ,   0.  ],\n",
       "       [  4.7 ,  90.13,  19.  , ...,   1.  ,   0.  ,   0.  ],\n",
       "       [  5.8 ,  89.25,  23.  , ...,   2.  ,   0.  ,   1.  ],\n",
       "       ...,\n",
       "       [  3.1 ,  96.03,  15.  , ...,   0.  ,   0.  ,   0.  ],\n",
       "       [  0.6 , 103.05,   8.  , ...,   0.  ,   0.  ,   0.  ],\n",
       "       [  9.8 ,  55.22,  27.  , ...,   0.  ,   1.  ,   0.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 11166)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11166"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "# for k,v in vocab.iteritems():\n",
    "#     variables[v] = k\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "# for k,v in pos_vocab.iteritems():\n",
    "#     pos_variables[v] = k\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "This model was found using a GridSearch with 5-fold cross validation. Details are in the notebook operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 175)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.57      0.51      1430\n",
      "           1       0.97      0.92      0.94     19190\n",
      "           2       0.83      0.96      0.89      4163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     24783\n",
      "   macro avg       0.75      0.82      0.78     24783\n",
      "weighted avg       0.91      0.90      0.91     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using information from the model to obtain the matrix X_ generically\n",
    "\n",
    "This is the most difficult task: We have to take the inputs tweets and transform them into a format that can be used in the model without going through all the same pre-processing steps as above. This can be done as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = select.get_support(indices=True) #get indices of features\n",
    "# final_feature_list = [unicode(feature_names[i]) for i in final_features] #Get list of names corresponding to indices\n",
    "final_feature_list = [str(feature_names[i]) for i in final_features] #Get list of names corresponding to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['america', 'american', 'anoth', 'ass', 'ass cracker', 'ass hoe', 'ass nigga', 'bad', 'beaner', 'big', 'bird', 'bitch', 'bitch nigga', 'black', 'border', 'born', 'bout', 'browni', 'campu', 'charli', 'chink', 'color', 'color folk', 'coon', 'countri', 'cracker', 'crazi', 'crippl', 'cunt', 'da', 'damn', 'darki', 'dick', 'die', 'doe', 'dyke', 'fag', 'faggot', 'fat', 'femal', 'feminist', 'filth', 'first', 'folk', 'fucc nicca', 'fuck', 'fuckin', 'game', 'gay', 'get', 'ghetto', 'girl', 'gon', 'good', 'gook', 'got nigga', 'gt gt', 'hate', 'hate hoe', 'hi', 'hire', 'ho', 'hoe', 'hood', 'hope', 'human', 'israel', 'jap', 'jew', 'jihadi', 'kill', 'lame', 'latina', 'let', 'like', 'lol', 'look like', 'love', 'may', 'mexican', 'mock', 'money', 'monkey', 'much', 'muslim', 'muzzi', 'negro', 'nicca', 'nig', 'nigga', 'nigga bitch', 'niggah', 'niggaz', 'nigger', 'nigguh', 'niglet', 'oreo', 'peopl', 'play', 'pussi', 'queer', 'race', 'racist', 'real', 'real nigga', 'redneck', 'retard', 'sex', 'shit', 'shoot', 'show', 'shut', 'side', 'slope', 'smh', 'sole', 'special', 'spic', 'stfu', 'stupid', 'suck', 'tcot', 'teabagg', 'teach', 'thank', 'thi pussi', 'tho', 'throat', 'towel', 'trailer', 'tranni', 'trash', 'tri', 'twat', 'u', 'ugli', 'uncl', 'ur', 'us', 'use', 'wassup', 'watch', 'wear', 'wetback', 'whi', 'white', 'white trash', 'whitey', 'whore', 'word', 'ya', 'yanke', 'yeah', 'yellow', 'yr', 'zimmerman', 'IN NN', 'JJ NN', 'NN NN NN', 'NNS', 'PRP VBP', 'RB', 'VB', 'VBD', 'FKRA', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader compound', 'num_hashtags', 'num_mentions']\n"
     ]
    }
   ],
   "source": [
    "# print final_feature_list\n",
    "print(final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting names for each class of features\n",
    "ngram_features = final_feature_list[:final_feature_list.index('zimmerman')+1]\n",
    "pos_features = final_feature_list[final_feature_list.index('zimmerman')+1:final_feature_list.index('VBD')+1]\n",
    "oth_features = final_feature_list[final_feature_list.index('VBD')+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FKRA',\n",
       " 'FRE',\n",
       " 'num_syllables',\n",
       " 'num_chars',\n",
       " 'num_chars_total',\n",
       " 'num_terms',\n",
       " 'num_words',\n",
       " 'num_unique_words',\n",
       " 'vader compound',\n",
       " 'num_hashtags',\n",
       " 'num_mentions']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oth_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {v:i for i, v in enumerate(ngram_features)}\n",
    "new_vocab_to_index = {}\n",
    "for k in ngram_features:\n",
    "    new_vocab_to_index[k] = vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'america': 126,\n",
       " 'american': 127,\n",
       " 'anoth': 180,\n",
       " 'ass': 250,\n",
       " 'ass cracker': 260,\n",
       " 'ass hoe': 269,\n",
       " 'ass nigga': 277,\n",
       " 'bad': 356,\n",
       " 'beaner': 416,\n",
       " 'big': 493,\n",
       " 'bird': 512,\n",
       " 'bitch': 535,\n",
       " 'bitch nigga': 753,\n",
       " 'black': 893,\n",
       " 'border': 958,\n",
       " 'born': 960,\n",
       " 'bout': 973,\n",
       " 'browni': 1047,\n",
       " 'campu': 1132,\n",
       " 'charli': 1209,\n",
       " 'chink': 1253,\n",
       " 'color': 1332,\n",
       " 'color folk': 1335,\n",
       " 'coon': 1408,\n",
       " 'countri': 1433,\n",
       " 'cracker': 1444,\n",
       " 'crazi': 1450,\n",
       " 'crippl': 1470,\n",
       " 'cunt': 1499,\n",
       " 'da': 1525,\n",
       " 'damn': 1544,\n",
       " 'darki': 1561,\n",
       " 'dick': 1665,\n",
       " 'die': 1674,\n",
       " 'doe': 1721,\n",
       " 'dyke': 1820,\n",
       " 'fag': 2000,\n",
       " 'faggot': 2002,\n",
       " 'fat': 2040,\n",
       " 'femal': 2081,\n",
       " 'feminist': 2084,\n",
       " 'filth': 2099,\n",
       " 'first': 2117,\n",
       " 'folk': 2168,\n",
       " 'fucc nicca': 2245,\n",
       " 'fuck': 2246,\n",
       " 'fuckin': 2332,\n",
       " 'game': 2366,\n",
       " 'gay': 2379,\n",
       " 'get': 2390,\n",
       " 'ghetto': 2470,\n",
       " 'girl': 2485,\n",
       " 'gon': 2580,\n",
       " 'good': 2599,\n",
       " 'gook': 2620,\n",
       " 'got nigga': 2658,\n",
       " 'gt gt': 2728,\n",
       " 'hate': 2801,\n",
       " 'hate hoe': 2809,\n",
       " 'hi': 2859,\n",
       " 'hire': 2898,\n",
       " 'ho': 2911,\n",
       " 'hoe': 2916,\n",
       " 'hood': 3085,\n",
       " 'hope': 3094,\n",
       " 'human': 3128,\n",
       " 'israel': 3275,\n",
       " 'jap': 3298,\n",
       " 'jew': 3317,\n",
       " 'jihadi': 3323,\n",
       " 'kill': 3399,\n",
       " 'lame': 3486,\n",
       " 'latina': 3511,\n",
       " 'let': 3554,\n",
       " 'like': 3588,\n",
       " 'lol': 3756,\n",
       " 'look like': 3787,\n",
       " 'love': 3816,\n",
       " 'may': 3986,\n",
       " 'mexican': 4026,\n",
       " 'mock': 4095,\n",
       " 'money': 4109,\n",
       " 'monkey': 4115,\n",
       " 'much': 4149,\n",
       " 'muslim': 4165,\n",
       " 'muzzi': 4172,\n",
       " 'negro': 4237,\n",
       " 'nicca': 4291,\n",
       " 'nig': 4304,\n",
       " 'nigga': 4307,\n",
       " 'nigga bitch': 4314,\n",
       " 'niggah': 4386,\n",
       " 'niggaz': 4392,\n",
       " 'nigger': 4394,\n",
       " 'nigguh': 4401,\n",
       " 'niglet': 4405,\n",
       " 'oreo': 4573,\n",
       " 'peopl': 4653,\n",
       " 'play': 4735,\n",
       " 'pussi': 4901,\n",
       " 'queer': 4988,\n",
       " 'race': 5002,\n",
       " 'racist': 5006,\n",
       " 'real': 5050,\n",
       " 'real nigga': 5054,\n",
       " 'redneck': 5095,\n",
       " 'retard': 5148,\n",
       " 'sex': 5399,\n",
       " 'shit': 5430,\n",
       " 'shoot': 5467,\n",
       " 'show': 5478,\n",
       " 'shut': 5484,\n",
       " 'side': 5497,\n",
       " 'slope': 5564,\n",
       " 'smh': 5585,\n",
       " 'sole': 5624,\n",
       " 'special': 5689,\n",
       " 'spic': 5698,\n",
       " 'stfu': 5762,\n",
       " 'stupid': 5842,\n",
       " 'suck': 5858,\n",
       " 'tcot': 5987,\n",
       " 'teabagg': 5993,\n",
       " 'teach': 5995,\n",
       " 'thank': 6054,\n",
       " 'thi pussi': 6124,\n",
       " 'tho': 6183,\n",
       " 'throat': 6202,\n",
       " 'towel': 6289,\n",
       " 'trailer': 6295,\n",
       " 'tranni': 6300,\n",
       " 'trash': 6305,\n",
       " 'tri': 6335,\n",
       " 'twat': 6397,\n",
       " 'u': 6441,\n",
       " 'ugli': 6507,\n",
       " 'uncl': 6524,\n",
       " 'ur': 6548,\n",
       " 'us': 6558,\n",
       " 'use': 6563,\n",
       " 'wassup': 6718,\n",
       " 'watch': 6722,\n",
       " 'wear': 6743,\n",
       " 'wetback': 6779,\n",
       " 'whi': 6786,\n",
       " 'white': 6818,\n",
       " 'white trash': 6828,\n",
       " 'whitey': 6830,\n",
       " 'whore': 6838,\n",
       " 'word': 6908,\n",
       " 'ya': 6976,\n",
       " 'yanke': 6998,\n",
       " 'yeah': 7017,\n",
       " 'yellow': 7029,\n",
       " 'yr': 7074,\n",
       " 'zimmerman': 7083}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of text features\n",
    "ngram_indices = final_features[:len(ngram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle new vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_tfidf.pkl']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(new_vectorizer, 'final_tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf_ = new_vectorizer.fit_transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 4.80981477, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.81738461, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.627199378751962"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are the same if use IDF but the problem is that IDF will be different if we use different data. Instead we have to use the original IDF scores and multiply them by the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vals_ = idf_vals[ngram_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_idf.pkl']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Pickle idf_vals\n",
    "\n",
    "joblib.dump(idf_vals_, 'final_idf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_[1,:]*idf_vals_) == X_[1,:153] #Got same value as final process array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_*idf_vals_ == X_[:,:153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffinal = tfidf_*idf_vals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating POS features\n",
    "This is simpler as we do not need to worry about IDF but it will be slower as we have to compute the POS tags for the new data. Here we can simply use the old POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = {v:i for i, v in enumerate(pos_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle pos vectorizer\n",
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "new_pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_pos.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_pos_vectorizer, 'final_pos.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = new_pos_vectorizer.fit_transform(tweet_tags).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:] == X_[:,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95689.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33411.363676851324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[:,153:159].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can look at the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neg', 'vader pos', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls', 'is_retweet']\n"
     ]
    }
   ],
   "source": [
    "# print other_features_names\n",
    "print(other_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader compound', 'num_hashtags', 'num_mentions']\n"
     ]
    }
   ],
   "source": [
    "# print oth_features\n",
    "print(oth_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions can be modified to only calculate and return necessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array_(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_ = get_feature_array_(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3   ,  79.94  ,  30.    , 127.    , 140.    ,  25.    ,\n",
       "        25.    ,  23.    ,   0.4563,   0.    ,   1.    ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.    ,   1.    ,   2.    ,   1.    ,   0.    ,   8.3   ,\n",
       "        79.94  ,  30.    , 127.    , 140.    ,  25.    ,  25.    ,\n",
       "        23.    ,   0.4563,   0.    ,   1.    ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,159:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[:,:] == X_[:,159:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have put it all together using a simplified process we can assess if these new data return the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.concatenate([tfidffinal, pos_, feats_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 175)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = pd.DataFrame(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_ = model.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.56      0.51      1430\n",
      "           1       0.97      0.92      0.94     19190\n",
      "           2       0.82      0.96      0.89      4163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     24783\n",
      "   macro avg       0.75      0.81      0.78     24783\n",
      "weighted avg       0.91      0.90      0.91     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So now that we have verified that the results are the same with X_ and X__ we can implement a script that can transform new data in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
