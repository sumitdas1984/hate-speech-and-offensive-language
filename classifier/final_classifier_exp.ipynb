{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to generate the finalized version of the classifier, to simply feature transformation into the final form, and to test that the results are the same\n",
    "\n",
    "Most of the code comes from operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "#Loading raw data\n",
    "# df = pickle.load(open(\"../Data/multiclass_tweets_indexed.p\",'rb'))\n",
    "# df = pickle.load(open(\"../Data/labeled_data.p\",'rb'))\n",
    "df = pd.read_pickle(\"../Data/labeled_data.p\")\n",
    "# tweets = df.text\n",
    "tweets = df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither class  \\\n",
       "0      3            0                   0        3     2   \n",
       "1      3            0                   3        0     1   \n",
       "2      3            0                   3        0     1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3        !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "5        !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
       "6        !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
       "7        !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
       "8        \" &amp; you might not get ya bitch back &amp; ...\n",
       "9        \" @rhythmixx_ :hobbies include: fighting Maria...\n",
       "10       \" Keeks is a bitch she curves everyone \" lol I...\n",
       "11                      \" Murda Gang bitch its Gang Land \"\n",
       "12       \" So hoes that smoke are losers ? \" yea ... go...\n",
       "13           \" bad bitches is the only thing that i like \"\n",
       "14                                 \" bitch get up off me \"\n",
       "15                         \" bitch nigga miss me with it \"\n",
       "16                                  \" bitch plz whatever \"\n",
       "17                               \" bitch who do you love \"\n",
       "18                      \" bitches get cut off everyday B \"\n",
       "19                      \" black bottle &amp; a bad bitch \"\n",
       "20                    \" broke bitch cant tell me nothing \"\n",
       "21                         \" cancel that bitch like Nino \"\n",
       "22                 \" cant you see these hoes wont change \"\n",
       "23       \" fuck no that bitch dont even suck dick \" &#1...\n",
       "24       \" got ya bitch tip toeing on my hardwood floor...\n",
       "25          \" her pussy lips like Heaven doors \" &#128524;\n",
       "26                            \" hoe what its hitting for \"\n",
       "27       \" i met that pussy on Ocean Dr . i gave that p...\n",
       "28          \" i need a trippy bitch who fuck on Hennessy \"\n",
       "29       \" i spend my money how i want bitch its my bus...\n",
       "                               ...                        \n",
       "25266               you ain't gotta be a dyke to like hoes\n",
       "25267                     you are a hoe, hoe, &amp; a hoe.\n",
       "25268               you bitches love yall some corny nigga\n",
       "25269    you can masturbate anytime bitch lol &#8220;@g...\n",
       "25270    you can never get a group of hoes together wit...\n",
       "25271    you can tell when dick recently been in a puss...\n",
       "25272                            you can't cuff a hoe lmao\n",
       "25273                           you drove me redneck crazy\n",
       "25274                                you fake niggah lolol\n",
       "25275                   you got niggas, and i got bitches.\n",
       "25276    you gotta be a new breed of retarded if you do...\n",
       "25277    you gotta understand that these bitches are ch...\n",
       "25278                                        you hoe spice\n",
       "25279                     you just want some attention hoe\n",
       "25280    you know what they say, the early bird gets th...\n",
       "25281    you know what your doing when you favorite a t...\n",
       "25282    you lil dumb ass bitch, i ain't fuckin wit chu...\n",
       "25283    you look like AC Green...bitch don't call here...\n",
       "25284    you look like your 12 stop talking about fucki...\n",
       "25285          you might as well gone pussy pop on a stage\n",
       "25286                you niggers cheat on ya gf's? smh....\n",
       "25287    you really care bout dis bitch. my dick all in...\n",
       "25288     you worried bout other bitches, you need me for?\n",
       "25289                                   you're all niggers\n",
       "25290    you're such a retard i hope you get type 2 dia...\n",
       "25291    you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "25292    you've gone and broke the wrong heart baby, an...\n",
       "25294    young buck wanna eat!!.. dat nigguh like I ain...\n",
       "25295                youu got wild bitches tellin you lies\n",
       "25296    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df.tweet\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "# idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict = {}\n",
    "# for i in vocab.values():\n",
    "#     print(str(i) + '\\t\\t' + str(idf_vals[i]))\n",
    "#     idf_dict[i] = idf_vals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab\n",
    "\n",
    "# for i, v in enumerate(vectorizer.get_feature_names()):\n",
    "#     print(str(i) + '\\t\\t' + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in vocab.values():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "#     print(tags)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        RB NN IN DT NN PRP VBP JJ NN IN VBG RP PRP$ NN...\n",
       "1           JJ NN NN NNS VBP JJ JJ IN NN NN NN IN DT NN NN\n",
       "2        JJ NN NN NN PRP RB VBD DT NN CC PRP VBP TO VB ...\n",
       "3                                  NN VBZ PRP VBP IN DT NN\n",
       "4        NN VBZ DT NN PRP VBP IN PRP MD VB JJ CC PRP MD...\n",
       "5        IN DT NN RB VBZ NN PRP RB JJ CC RB IN NN CC RB...\n",
       "6        NN NN MD RB RB VB RP CC VB IN DT NN NN NN VBD ...\n",
       "7        NN NN NN VBP VBN IN PRP JJ NNS VBG IN PRP VBP NNS\n",
       "8                      NN PRP MD RB VB JJ VB RB JJ NNS WDT\n",
       "9                                       NNS VBP VBG NNS NN\n",
       "10       NN VBZ DT NN PRP VBZ NN NN NN VBD IN DT NN IN ...\n",
       "11                                     NN NN VB PRP$ NN NN\n",
       "12                   RB VBZ IN NN VBP NNS . NN : VBP IN NN\n",
       "13                            JJ NNS VBZ DT JJ NN IN NN IN\n",
       "14                                         NN VB RP IN PRP\n",
       "15                                     NN NN VB PRP IN PRP\n",
       "16                                               NN NN WDT\n",
       "17                                        NN WP VBP PRP VB\n",
       "18                                    NNS VBP VBN RP JJ NN\n",
       "19                                      JJ NN VBD DT JJ NN\n",
       "20                                     VBD NN NN VB PRP NN\n",
       "21                                        NN WDT VBP IN NN\n",
       "22                                NN PRP VBP DT NNS VBP NN\n",
       "23       NN DT WDT VBP VB RB JJ VBP DT NN NN NN TO VB V...\n",
       "24                        VBD PRP VB NN VBG IN PRP$ NN NNS\n",
       "25                                   PRP$ JJ NNS IN NN NNS\n",
       "26                                        VB WP PRP$ NN IN\n",
       "27              NN VBD IN NN IN JJ NN . NN VBD IN NN DT NN\n",
       "28                           NNS VBP DT JJ NN WP VBP IN NN\n",
       "29               JJ VBP PRP$ NN WRB NN VBP NN PRP$ PRP$ NN\n",
       "                               ...                        \n",
       "24753                     PRP VBP JJ NN VB DT NN TO VB NNS\n",
       "24754                           PRP VBP DT JJ NN VBZ DT NN\n",
       "24755                               PRP VBP VB CC DT NN NN\n",
       "24756      PRP MD VB JJ NN NN NN RB VB DT NN RB JJ NNS VBP\n",
       "24757     PRP MD RB VB DT NN IN NNS RB IN NN VBG TO VBG NN\n",
       "24758    PRP MD VB WRB NN RB VBN IN DT JJ NN JJ NN NN N...\n",
       "24759                                PRP MD VB VB DT NN NN\n",
       "24760                                     PRP VB PRP VB JJ\n",
       "24761                                        PRP VBP JJ NN\n",
       "24762                             PRP VBD JJ CC JJ VBD NNS\n",
       "24763              PRP VBP VB DT JJ NN IN JJ IN PRP VBP VB\n",
       "24764                         PRP VBP VBP IN DT NNS VBP JJ\n",
       "24765                                          PRP VBP NNS\n",
       "24766                                  PRP RB VBP DT NN NN\n",
       "24767    PRP VBP WP PRP VBP DT JJ NN VBZ DT NN VBZ JJ N...\n",
       "24768    PRP VBP WP PRP$ VBG WRB PRP VBP DT NN NN NN NN...\n",
       "24769    PRP VBP JJ NN NN NN VBP JJ JJ NN NN NN VBD DT ...\n",
       "24770                    PRP VBP IN NNS VBP VB NN NN RB NN\n",
       "24771                    PRP VBP IN PRP$ NN VBG IN VBG NNS\n",
       "24772                      PRP MD RB RB VBN JJ NN IN DT NN\n",
       "24773                           PRP NNS VBP IN NN NN NN NN\n",
       "24774              PRP RB VB IN NN NN PRP$ NN DT IN JJ NNS\n",
       "24775                      PRP VBP IN JJ NN PRP VBP PRP VB\n",
       "24776                                       PRP VBP DT NNS\n",
       "24777    PRP VBP PDT DT NN NN VBP PRP VBP JJ NNS CC NN ...\n",
       "24778    PRP VBP DT NN IN NN VB PRP$ NN VBZ JJ . JJ NN ...\n",
       "24779       PRP VBP VBN CC VBD DT JJ NN NN CC VB PRP VB JJ\n",
       "24780            JJ NN JJ NN NN IN IN JJ VBP NNS VBP RP RB\n",
       "24781                            NN VBD JJ NNS VBP PRP NNS\n",
       "24782      VBN RB JJ NNS JJ NN NN IN NN NN JJ NN VBD DT NN\n",
       "Length: 24783, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tweet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD NN RB': 205,\n",
       " 'TO DT NN': 2460,\n",
       " 'VBD JJ VB': 2854,\n",
       " 'NNS VBZ JJ': 1746,\n",
       " 'CD NNS RB': 216,\n",
       " 'RB NN JJ': 2146,\n",
       " 'JJ IN VBG': 773,\n",
       " 'CC NN NNS': 51,\n",
       " 'RB VBD JJ': 2243,\n",
       " 'JJ CC DT': 732,\n",
       " 'WP VBZ PRP': 3957,\n",
       " 'EX MD': 446,\n",
       " 'IN DT JJS': 502,\n",
       " 'VB NN IN': 2619,\n",
       " 'MD VB JJR': 1097,\n",
       " 'CD NN MD': 201,\n",
       " 'DT PRP JJ': 339,\n",
       " 'NN RP PRP': 1339,\n",
       " 'EX VBZ NN': 460,\n",
       " 'EX VBZ DT': 459,\n",
       " 'CD CC': 182,\n",
       " 'WP VBD DT': 3926,\n",
       " 'NNS VBP VBZ': 1740,\n",
       " 'VBZ WP': 3796,\n",
       " 'RP DT NNS': 2375,\n",
       " 'VB DT RBR': 2560,\n",
       " 'WRB MD': 3992,\n",
       " 'VBZ WP NN': 3799,\n",
       " 'CC PRP NN': 81,\n",
       " 'CC VBD PRP': 135,\n",
       " 'RB VBP RP': 2292,\n",
       " 'DT VBP TO': 415,\n",
       " 'CC VB JJ': 113,\n",
       " 'VBZ RB IN': 3708,\n",
       " 'VBD EX': 2827,\n",
       " 'NN WP DT': 1485,\n",
       " 'RBS JJ NN': 2360,\n",
       " 'VBD IN CC': 2829,\n",
       " 'VBZ IN IN': 3626,\n",
       " 'WP VBD IN': 3927,\n",
       " 'IN NN VBG': 588,\n",
       " 'RB VBN VB': 2277,\n",
       " 'VBP IN WP': 3367,\n",
       " 'VBP IN EX': 3351,\n",
       " 'RB VBP CD': 2283,\n",
       " 'JJ WRB NNS': 982,\n",
       " 'VBD NN WDT': 2881,\n",
       " 'DT VBN': 401,\n",
       " 'CC NN RB': 53,\n",
       " 'NNS IN': 1554,\n",
       " 'RBR IN PRP': 2336,\n",
       " 'NN JJ DT': 1190,\n",
       " 'PRP CC PRP': 1783,\n",
       " 'WRB NN VBZ': 4009,\n",
       " 'WP PRP VBP': 3909,\n",
       " 'VBP VBZ PRP': 3574,\n",
       " 'VBP VB TO': 3510,\n",
       " 'RB VBD IN': 2242,\n",
       " 'JJ NN VBP': 827,\n",
       " 'NN NN IN': 1227,\n",
       " 'JJ IN WRB': 776,\n",
       " 'VBD IN IN': 2833,\n",
       " 'DT NNS TO': 328,\n",
       " 'NN TO RB': 1349,\n",
       " 'VBN PRP DT': 3237,\n",
       " 'VBD WP PRP': 3003,\n",
       " 'WRB VBZ VB': 4062,\n",
       " 'VBZ VBD PRP': 3754,\n",
       " 'CC VB WP': 127,\n",
       " 'VBZ IN NN': 3629,\n",
       " 'CC VBD JJ': 132,\n",
       " 'IN JJS NNS': 564,\n",
       " 'VBG NN WRB': 3078,\n",
       " 'UH VB': 2534,\n",
       " 'PRP WRB JJ': 2063,\n",
       " 'VBP VBD PRP': 3524,\n",
       " 'NNS VBZ VBN': 1751,\n",
       " 'JJS JJ NNS': 1039,\n",
       " 'RBR VBG': 2354,\n",
       " 'IN PRP VB': 634,\n",
       " 'VBP VBP TO': 3564,\n",
       " 'JJS NNS IN': 1054,\n",
       " 'VBN DT IN': 3181,\n",
       " 'PRP CC VB': 1785,\n",
       " 'VB JJ VBP': 2603,\n",
       " 'RB VBG VBN': 2265,\n",
       " 'VBD NNS JJ': 2889,\n",
       " 'VBN DT': 3180,\n",
       " 'CC VBP VBG': 162,\n",
       " 'RB NN VBD': 2155,\n",
       " 'CC NNS MD': 66,\n",
       " 'RB VBZ MD': 2305,\n",
       " 'CC VB JJR': 114,\n",
       " 'RBR IN DT': 2333,\n",
       " 'VBN VBD': 3283,\n",
       " 'VBP NN VBD': 3411,\n",
       " 'DT NN RB': 302,\n",
       " 'NN NN PDT': 1235,\n",
       " 'DT VB VBG': 380,\n",
       " 'RP RB DT': 2420,\n",
       " 'NN JJS JJ': 1211,\n",
       " 'RP IN NNS': 2381,\n",
       " 'NNS RB TO': 1645,\n",
       " 'VB VBG JJ': 2742,\n",
       " 'VBP RP RB': 3486,\n",
       " 'JJR VB': 1025,\n",
       " 'DT JJ CD': 254,\n",
       " 'VBP VBG IN': 3528,\n",
       " 'IN PRP JJS': 626,\n",
       " 'VBN RP NNS': 3267,\n",
       " 'NNS RBR RB': 1656,\n",
       " 'JJ NN NN': 814,\n",
       " 'WRB PRP RB': 4026,\n",
       " 'VBP JJ CC': 3370,\n",
       " 'VBN IN PRP': 3194,\n",
       " 'VBZ PRP JJ': 3691,\n",
       " 'VBZ RB VBP': 3719,\n",
       " 'JJ JJ VBN': 790,\n",
       " 'VB JJR NNS': 2610,\n",
       " 'VBN JJ NN': 3204,\n",
       " 'MD VB JJ': 1096,\n",
       " 'NNS NNS NNS': 1610,\n",
       " 'MD VB': 1091,\n",
       " 'NN RB NNS': 1312,\n",
       " 'CC DT NN': 8,\n",
       " 'IN JJS VB': 567,\n",
       " 'VBZ JJ NNS': 3642,\n",
       " 'IN VBZ RB': 712,\n",
       " 'PRP NNS VB': 1893,\n",
       " 'MD NN': 1075,\n",
       " 'VBZ PRP PRP': 3695,\n",
       " 'WDT CD': 3811,\n",
       " 'VBD DT PRP': 2823,\n",
       " 'PRP RB VB': 1929,\n",
       " 'RB NN TO': 2153,\n",
       " 'VB TO VB': 2712,\n",
       " 'VBD JJ IN': 2847,\n",
       " 'VBD VB TO': 2969,\n",
       " 'NN VBD NN': 1378,\n",
       " 'TO PRP IN': 2486,\n",
       " 'VBG IN WRB': 3040,\n",
       " 'IN WDT': 716,\n",
       " 'RB JJ VB': 2129,\n",
       " 'DT NNS VBN': 332,\n",
       " 'VBP JJ VBD': 3384,\n",
       " 'VB NNS IN': 2640,\n",
       " 'RBR NNS IN': 2345,\n",
       " 'PRP VBD VBG': 1985,\n",
       " 'CC JJ DT': 25,\n",
       " 'NN VBG JJ': 1399,\n",
       " 'PRP JJS': 1851,\n",
       " 'CC PRP VBG': 87,\n",
       " 'VBZ NN CC': 3659,\n",
       " 'WRB PRP CC': 4019,\n",
       " 'VB EX': 2569,\n",
       " 'CC WP': 173,\n",
       " 'VB PRP RB': 2666,\n",
       " 'PRP NN': 1859,\n",
       " 'PRP VBN PRP': 2007,\n",
       " 'VBP VBZ VB': 3576,\n",
       " 'VBN RP DT': 3263,\n",
       " 'JJ VB VBN': 910,\n",
       " 'VBP NN': 3399,\n",
       " 'JJ CD NN': 748,\n",
       " 'VBG NN NNS': 3067,\n",
       " 'IN JJ RB': 547,\n",
       " 'NN VBD VBP': 1391,\n",
       " 'VBG NN RB': 3069,\n",
       " 'JJS JJ VBP': 1040,\n",
       " 'NN PRP NNS': 1292,\n",
       " 'IN RB DT': 645,\n",
       " 'PRP PRP JJ': 1907,\n",
       " 'JJS TO VB': 1060,\n",
       " 'CC IN NNS': 20,\n",
       " 'NN RB DT': 1307,\n",
       " 'RB VBN': 2266,\n",
       " 'VBG TO DT': 3135,\n",
       " 'DT VBP RP': 414,\n",
       " 'TO': 2452,\n",
       " 'VBP JJS': 3395,\n",
       " 'IN PRP MD': 627,\n",
       " 'RB VB JJ': 2224,\n",
       " 'IN VBZ WRB': 715,\n",
       " 'CC JJ TO': 33,\n",
       " 'PRP VBD DT': 1973,\n",
       " 'DT NN JJR': 295,\n",
       " 'NN VBN JJ': 1416,\n",
       " 'DT JJS NNS': 283,\n",
       " 'RP NN CC': 2395,\n",
       " 'CC WRB NN': 178,\n",
       " 'NN NN JJS': 1230,\n",
       " 'IN NNS NN': 603,\n",
       " 'NN RP DT': 1335,\n",
       " 'JJS TO': 1059,\n",
       " 'VBZ PRP NNS': 3694,\n",
       " 'DT PRP NNS': 342,\n",
       " 'TO VB CD': 2499,\n",
       " 'PRP VBZ VB': 2048,\n",
       " 'NN NNS WDT': 1279,\n",
       " 'RB NNS': 2162,\n",
       " 'IN VBG TO': 688,\n",
       " 'VBD RB NNS': 2928,\n",
       " 'RB DT PRP': 2094,\n",
       " 'PRP JJ DT': 1829,\n",
       " 'RB VBZ VBG': 2314,\n",
       " 'VBZ DT JJS': 3611,\n",
       " 'RB NNS IN': 2164,\n",
       " 'CD NN VBZ': 209,\n",
       " 'VBD JJ JJ': 2848,\n",
       " 'JJS DT': 1032,\n",
       " 'VBG VBD': 3145,\n",
       " 'VBD VBN PRP': 2994,\n",
       " 'VBP VBN WRB': 3554,\n",
       " 'UH NN NN': 2529,\n",
       " 'VBP WRB TO': 3596,\n",
       " 'VBD VB': 2961,\n",
       " 'VBN IN WRB': 3198,\n",
       " 'FW FW FW': 466,\n",
       " 'WDT VBZ VBP': 3874,\n",
       " 'NNS NN JJ': 1592,\n",
       " 'NN TO VBG': 1351,\n",
       " 'DT VB RP': 379,\n",
       " 'VBN PRP TO': 3244,\n",
       " 'VBG VBN CC': 3155,\n",
       " 'JJS IN NN': 1035,\n",
       " 'NN RP NN': 1338,\n",
       " 'VBP IN TO': 3361,\n",
       " 'IN FW': 521,\n",
       " 'VBD JJ VBG': 2856,\n",
       " 'NNP NN CC': 1512,\n",
       " 'DT VBD WRB': 393,\n",
       " 'CC PRP RB': 84,\n",
       " 'VBP VBG CC': 3526,\n",
       " 'WP VBZ TO': 3960,\n",
       " 'VBD NNS': 2886,\n",
       " 'DT VB': 374,\n",
       " 'DT NNS MD': 321,\n",
       " 'RB JJ NN': 2124,\n",
       " 'VBD PRP MD': 2909,\n",
       " 'WDT NN VBZ': 3825,\n",
       " 'NNS RP IN': 1660,\n",
       " 'VBD CC JJ': 2805,\n",
       " 'NNS VBZ NNS': 1747,\n",
       " 'VB DT DT': 2551,\n",
       " 'RB TO PRP': 2216,\n",
       " 'NN FW': 1159,\n",
       " 'MD PRP RB': 1080,\n",
       " 'VBN TO NN': 3273,\n",
       " 'JJ VBZ PRP': 971,\n",
       " 'IN NN VBN': 589,\n",
       " 'VBN IN RB': 3195,\n",
       " 'NN VBP VBG': 1444,\n",
       " 'NNS NNS VBD': 1615,\n",
       " 'JJR JJ': 1002,\n",
       " 'DT VBN NN': 403,\n",
       " 'NNS CC VBN': 1540,\n",
       " 'DT VBD NN': 386,\n",
       " 'NN JJ TO': 1199,\n",
       " 'PRP WRB DT': 2062,\n",
       " 'PRP NNS NN': 1887,\n",
       " 'VB NNS CC': 2638,\n",
       " 'NNS VBN JJ': 1713,\n",
       " 'VBG PRP VBP': 3110,\n",
       " 'VBP PRP IN': 3442,\n",
       " 'JJ VBD NN': 919,\n",
       " 'RP PRP NNS': 2416,\n",
       " 'IN VBP RP': 705,\n",
       " 'NN JJ JJR': 1193,\n",
       " 'VB NN WRB': 2635,\n",
       " 'JJ MD VB': 802,\n",
       " 'VBP EX VBZ': 3346,\n",
       " 'JJ CC': 731,\n",
       " 'NN TO NN': 1346,\n",
       " 'NNS VBP TO': 1734,\n",
       " 'VBP VB CD': 3501,\n",
       " 'PRP VBN VBD': 2009,\n",
       " 'VB JJ CD': 2589,\n",
       " 'TO JJ NNS': 2469,\n",
       " 'JJ TO NN': 894,\n",
       " 'VBP PRP PRP': 3448,\n",
       " 'CD TO': 221,\n",
       " 'NN PRP TO': 1296,\n",
       " 'NN VBG NNS': 1402,\n",
       " 'VBN': 3169,\n",
       " 'VBZ TO JJ': 3739,\n",
       " 'WRB PDT': 4016,\n",
       " 'NN WDT CD': 1474,\n",
       " 'VBG NNS IN': 3081,\n",
       " 'JJS VB DT': 1062,\n",
       " 'PRP CC NN': 1782,\n",
       " 'RP WRB DT': 2449,\n",
       " 'RB VBZ NN': 2306,\n",
       " 'JJS CC': 1030,\n",
       " 'MD PRP NN': 1079,\n",
       " 'NN RB VBG': 1319,\n",
       " 'NNS VBP NNS': 1729,\n",
       " 'NN WP VBP': 1493,\n",
       " 'PRP DT VBD': 1804,\n",
       " 'PRP JJR VBD': 1850,\n",
       " 'VBZ VBN CC': 3768,\n",
       " 'VBZ RBR': 3722,\n",
       " 'CC VB MD': 115,\n",
       " 'VBG JJ NNS': 3047,\n",
       " 'NN VBG RP': 1405,\n",
       " 'NNS VBP VBN': 1738,\n",
       " 'DT NNS VBG': 331,\n",
       " 'NN JJR': 1206,\n",
       " 'VB DT RBS': 2561,\n",
       " 'VBG PRP VBG': 3109,\n",
       " 'VB CC NN': 2539,\n",
       " 'VBG VBZ': 3163,\n",
       " 'VBG CD IN': 3018,\n",
       " 'NNS EX': 1552,\n",
       " 'VBG NN MD': 3065,\n",
       " 'VB NN RB': 2625,\n",
       " 'NN VBZ VBZ': 1470,\n",
       " 'JJ VBP': 944,\n",
       " 'NNS WRB JJ': 1768,\n",
       " 'PRP VBD VBD': 1984,\n",
       " 'IN TO VBG': 666,\n",
       " 'VBN VBD NN': 3286,\n",
       " 'RB VBD TO': 2249,\n",
       " 'JJ DT NNS': 754,\n",
       " 'VBG NNS WRB': 3092,\n",
       " 'VBP IN IN': 3352,\n",
       " 'NNS VBN PRP': 1716,\n",
       " 'VBP NN WDT': 3416,\n",
       " 'IN VBN RB': 695,\n",
       " 'JJ VBP VBN': 960,\n",
       " 'VBZ VBN NN': 3772,\n",
       " 'VBZ VBG CC': 3756,\n",
       " 'VBZ VBG': 3755,\n",
       " 'RB VBP WRB': 2300,\n",
       " 'VB DT CC': 2549,\n",
       " 'PRP NNS DT': 1883,\n",
       " 'MD VB TO': 1103,\n",
       " 'VBZ MD': 3656,\n",
       " 'NN RB VBP': 1321,\n",
       " 'MD VB IN': 1095,\n",
       " 'VBZ TO VB': 3741,\n",
       " 'NN JJ VBP': 1204,\n",
       " 'WRB VBZ NN': 4059,\n",
       " 'JJ VBG NN': 930,\n",
       " 'NN WDT NN': 1478,\n",
       " 'VB WRB TO': 2798,\n",
       " 'RB JJ PRP': 2126,\n",
       " 'DT NNS RBR': 326,\n",
       " 'NN DT JJR': 1142,\n",
       " 'CC JJ NNS': 30,\n",
       " 'NN VBD CC': 1372,\n",
       " 'RB NNS RB': 2166,\n",
       " 'WRB VBZ PRP': 4060,\n",
       " 'VBP DT NN': 3333,\n",
       " 'PRP PRP RB': 1911,\n",
       " 'RP VB PRP': 2436,\n",
       " 'CC JJ VB': 34,\n",
       " 'DT RB PRP': 354,\n",
       " 'NNS VBP RB': 1732,\n",
       " 'CC RB IN': 93,\n",
       " 'NNS PRP': 1621,\n",
       " 'RB VBP DT': 2284,\n",
       " 'DT RB VBP': 360,\n",
       " 'VBN CC VB': 3175,\n",
       " 'VB NNS RP': 2645,\n",
       " 'DT VBD WP': 392,\n",
       " 'NN NN NNP': 1233,\n",
       " 'RB PRP JJ': 2177,\n",
       " 'JJR JJ IN': 1003,\n",
       " 'NNS JJS': 1584,\n",
       " 'VBP TO NNS': 3494,\n",
       " 'VBG IN JJ': 3032,\n",
       " 'NNS RB PRP': 1643,\n",
       " 'RBS NNS': 2363,\n",
       " 'VBZ JJ VBG': 3648,\n",
       " 'VB WRB VB': 2799,\n",
       " 'VBD JJ VBP': 2857,\n",
       " 'RBS': 2356,\n",
       " 'CC TO': 106,\n",
       " 'DT NNP': 315,\n",
       " 'VBZ NN IN': 3661,\n",
       " 'VBD NN CC': 2864,\n",
       " 'VBD TO NN': 2956,\n",
       " 'PRP CD JJ': 1791,\n",
       " 'IN EX VBP': 520,\n",
       " 'VB PRP VB': 2669,\n",
       " 'IN IN DT': 528,\n",
       " 'WRB RB DT': 4034,\n",
       " 'JJ VBP NNS': 952,\n",
       " 'VBD CD IN': 2811,\n",
       " 'IN PRP IN': 624,\n",
       " 'NN VBG NN': 1401,\n",
       " 'IN TO NN': 663,\n",
       " 'JJ NNP': 832,\n",
       " 'NNS IN CD': 1555,\n",
       " 'NN WRB VB': 1503,\n",
       " 'IN DT VBZ': 514,\n",
       " 'VBP WP IN': 3581,\n",
       " 'CC DT IN': 6,\n",
       " 'EX VBD DT': 453,\n",
       " 'WDT VBZ IN': 3862,\n",
       " 'DT RB VBZ': 361,\n",
       " 'VBG NNS PRP': 3085,\n",
       " 'VBN VBN': 3295,\n",
       " 'PRP FW': 1808,\n",
       " 'CC JJ VBZ': 38,\n",
       " 'DT RBR': 362,\n",
       " 'IN NN NNS': 581,\n",
       " 'VBN JJ CC': 3200,\n",
       " 'WRB VBP': 4048,\n",
       " 'RP TO': 2431,\n",
       " 'WDT VBZ VBG': 3872,\n",
       " 'CD CC VB': 184,\n",
       " 'VBD CC RB': 2807,\n",
       " 'CC DT VBD': 12,\n",
       " 'IN NNS PRP': 605,\n",
       " 'NN DT DT': 1139,\n",
       " 'WDT VBP VBN': 3858,\n",
       " 'VBP NNS NN': 3424,\n",
       " 'DT VBG IN': 396,\n",
       " 'IN PRP': 619,\n",
       " 'NN WRB VBZ': 1506,\n",
       " 'NN TO NNS': 1347,\n",
       " 'RP JJ VBP': 2393,\n",
       " 'NN DT CD': 1138,\n",
       " 'RB PRP VBD': 2183,\n",
       " 'JJ NNS VBN': 850,\n",
       " 'NN EX JJ': 1155,\n",
       " 'VBD PRP VBZ': 2920,\n",
       " 'VB JJ VBD': 2600,\n",
       " 'VBZ VBD IN': 3752,\n",
       " 'RB VBZ VB': 2312,\n",
       " 'VB JJR RB': 2611,\n",
       " 'VB IN VBG': 2584,\n",
       " 'VB RBR JJ': 2695,\n",
       " 'PRP VBG PRP': 1995,\n",
       " 'PRP VBP RB': 2025,\n",
       " 'VBD DT IN': 2818,\n",
       " 'NN VBN VBP': 1427,\n",
       " 'IN NN EX': 574,\n",
       " 'VBG JJ CC': 3042,\n",
       " 'PRP JJR IN': 1846,\n",
       " 'MD RB IN': 1084,\n",
       " 'NNS JJ NNS': 1576,\n",
       " 'CD JJ NNS': 193,\n",
       " 'IN JJ NNS': 545,\n",
       " 'RB CD NN': 2084,\n",
       " 'VBZ NNS VBG': 3683,\n",
       " 'VBP PDT': 3437,\n",
       " 'RP IN': 2376,\n",
       " 'VBG NNS CC': 3080,\n",
       " 'RP NNS VBG': 2409,\n",
       " 'NN VB CD': 1355,\n",
       " 'VBG NN CC': 3060,\n",
       " 'NN RB JJ': 1309,\n",
       " 'VBP VBP JJ': 3558,\n",
       " 'JJ CD NNS': 749,\n",
       " 'VBP WRB RB': 3595,\n",
       " 'WP PRP': 3904,\n",
       " 'WRB JJ JJ': 3981,\n",
       " 'VBP VBZ DT': 3570,\n",
       " 'RBR IN': 2332,\n",
       " 'FW NN PRP': 474,\n",
       " 'VBN RB VB': 3257,\n",
       " 'VBG NNS VBG': 3089,\n",
       " 'IN JJS RB': 566,\n",
       " 'VBD TO NNS': 2957,\n",
       " 'VBZ NN RB': 3667,\n",
       " 'JJ VBZ RB': 972,\n",
       " 'CC NN WDT': 60,\n",
       " 'NN IN PDT': 1176,\n",
       " 'JJ NNS JJ': 838,\n",
       " 'WRB JJ VBG': 3988,\n",
       " 'VBN RB': 3248,\n",
       " 'VBP VBP': 3555,\n",
       " 'VBG JJ NN': 3046,\n",
       " 'IN NN JJR': 578,\n",
       " 'VBD WP NN': 3002,\n",
       " 'VBN DT RB': 3185,\n",
       " 'VBP WRB DT': 3590,\n",
       " 'VBP NNS VBP': 3433,\n",
       " 'JJ JJ WRB': 793,\n",
       " 'VBN NN IN': 3216,\n",
       " 'NN VBG JJR': 1400,\n",
       " 'PRP VB PRP': 1960,\n",
       " 'PRP JJ VB': 1837,\n",
       " 'DT VB VBN': 381,\n",
       " 'NN NNS VBZ': 1278,\n",
       " 'VBG PRP NNS': 3102,\n",
       " 'WP IN': 3882,\n",
       " 'RB RB TO': 2199,\n",
       " 'NNS NN VBD': 1600,\n",
       " 'RB DT NN': 2092,\n",
       " 'VBN IN NN': 3192,\n",
       " 'VBZ DT VBZ': 3620,\n",
       " 'NN VBZ CC': 1451,\n",
       " 'WRB PRP NN': 4024,\n",
       " 'PRP JJS JJ': 1852,\n",
       " 'VBD JJ': 2843,\n",
       " 'DT NN CD': 289,\n",
       " 'JJ RB CC': 872,\n",
       " 'NNS RB': 1635,\n",
       " 'WDT VBD NNS': 3840,\n",
       " 'IN DT PRP': 506,\n",
       " 'JJ VBZ DT': 967,\n",
       " 'PRP VBD CC': 1971,\n",
       " 'NNP': 1507,\n",
       " 'NN VBZ PRP': 1460,\n",
       " 'NNS NNS IN': 1607,\n",
       " 'PRP VB JJ': 1956,\n",
       " 'VBP VBZ IN': 3571,\n",
       " 'JJ VBG RB': 933,\n",
       " 'VBD NN DT': 2865,\n",
       " 'EX IN': 443,\n",
       " 'VBZ DT RBR': 3616,\n",
       " 'IN NN DT': 573,\n",
       " 'VBG PRP RB': 3104,\n",
       " 'VBP IN JJ': 3353,\n",
       " 'PRP JJ VBP': 1841,\n",
       " 'DT WDT': 436,\n",
       " 'PRP VBP VBD': 2029,\n",
       " 'NN PRP VBP': 1301,\n",
       " 'IN JJ IN': 540,\n",
       " 'JJ NN WP': 830,\n",
       " 'JJ CC JJ': 734,\n",
       " 'RP JJ NNS': 2390,\n",
       " 'VBD NNP': 2884,\n",
       " 'RP VBG NNS': 2442,\n",
       " 'RB VBG RP': 2262,\n",
       " 'RB CC DT': 2070,\n",
       " 'DT NN RP': 304,\n",
       " 'RB DT': 2086,\n",
       " 'NNS VBZ RB': 1749,\n",
       " 'VBG JJ VB': 3051,\n",
       " 'VBD NN VBD': 2876,\n",
       " 'FW NNS': 475,\n",
       " 'DT NN': 287,\n",
       " 'MD VB VBG': 1106,\n",
       " 'RP VBG': 2438,\n",
       " 'NN MD RB': 1220,\n",
       " 'PRP NNS VBD': 1894,\n",
       " 'RB JJ VBG': 2131,\n",
       " 'PRP NN RBR': 1869,\n",
       " 'DT JJR JJ': 276,\n",
       " 'NNS TO': 1664,\n",
       " 'VBP MD VB': 3398,\n",
       " 'RB IN RB': 2109,\n",
       " 'VBP JJ TO': 3382,\n",
       " 'VBP CC': 3312,\n",
       " 'IN VBP NN': 703,\n",
       " 'JJ VBP RB': 955,\n",
       " 'WP RB': 3911,\n",
       " 'IN PRP RB': 631,\n",
       " 'VB VBD NN': 2735,\n",
       " 'NNS PDT DT': 1620,\n",
       " 'VB WP NN': 2783,\n",
       " 'NNS CC RB': 1536,\n",
       " 'PRP CC': 1778,\n",
       " 'VB VB VBN': 2727,\n",
       " 'RP NN PRP': 2401,\n",
       " 'WP NN MD': 3896,\n",
       " 'FW VB': 480,\n",
       " 'VBP IN VBG': 3363,\n",
       " 'JJ VBP TO': 957,\n",
       " 'DT CC PRP': 232,\n",
       " 'JJ VBP VBP': 961,\n",
       " 'DT TO': 371,\n",
       " 'CD JJ': 191,\n",
       " 'NN TO VB': 1350,\n",
       " 'VBP IN NN': 3356,\n",
       " 'VBN CC PRP': 3173,\n",
       " 'VBG NN TO': 3070,\n",
       " 'VB NN': 2616,\n",
       " 'CD DT NN': 186,\n",
       " 'VBP NN CC': 3400,\n",
       " 'JJ FW PRP': 761,\n",
       " 'WRB NN VBN': 4007,\n",
       " 'NN JJ MD': 1194,\n",
       " 'VB PDT': 2651,\n",
       " 'JJ VBP IN': 949,\n",
       " 'NNS VBP VBD': 1736,\n",
       " 'JJ NNP NN': 833,\n",
       " 'RB DT NNS': 2093,\n",
       " 'VBP VBG': 3525,\n",
       " 'VB VBP JJ': 2767,\n",
       " 'NNS NN MD': 1593,\n",
       " 'CC DT NNS': 9,\n",
       " 'NN RB MD': 1310,\n",
       " 'VB RB VBN': 2691,\n",
       " 'IN DT RBS': 508,\n",
       " 'PRP WRB TO': 2067,\n",
       " 'WP VBD NN': 3929,\n",
       " 'IN RBR': 658,\n",
       " 'VBZ EX DT': 3622,\n",
       " 'RB VBD DT': 2241,\n",
       " 'NN IN JJ': 1169,\n",
       " 'CC IN NN': 19,\n",
       " 'CC VBP IN': 154,\n",
       " 'DT RB VBG': 358,\n",
       " 'NN CC PRP': 1121,\n",
       " 'WRB NN': 3995,\n",
       " 'VBD RB TO': 2932,\n",
       " 'IN NN VBZ': 591,\n",
       " 'RB RB JJR': 2193,\n",
       " 'VBZ TO': 3737,\n",
       " 'DT VB DT': 375,\n",
       " 'VBD CC DT': 2803,\n",
       " 'NNP NN NN': 1515,\n",
       " 'NNS VB VBN': 1681,\n",
       " 'CC NN VB': 55,\n",
       " 'PRP NNS NNS': 1888,\n",
       " 'VBD RB VBD': 2934,\n",
       " 'NNS NN RB': 1597,\n",
       " 'VBD VBG CC': 2977,\n",
       " 'CC EX': 15,\n",
       " 'IN NN FW': 575,\n",
       " 'DT NNS VB': 329,\n",
       " 'VBN PRP VBP': 3246,\n",
       " 'RB DT JJS': 2091,\n",
       " 'NNP VBD': 1520,\n",
       " 'VBP DT DT': 3328,\n",
       " 'RB VB NN': 2226,\n",
       " 'VBN VB': 3277,\n",
       " 'WRB JJ': 3977,\n",
       " 'VBD VBN RB': 2995,\n",
       " 'TO VBG JJ': 2520,\n",
       " 'NNS PRP IN': 1623,\n",
       " 'WDT': 3809,\n",
       " 'VBD RB RB': 2930,\n",
       " 'VBD PRP JJS': 2908,\n",
       " 'RB RB VBP': 2204,\n",
       " 'NNS NNS VB': 1614,\n",
       " 'VBD DT DT': 2817,\n",
       " 'PRP IN VBG': 1823,\n",
       " 'JJ RB VBG': 884,\n",
       " 'VB IN NNS': 2578,\n",
       " 'MD TO': 1089,\n",
       " 'NN NNP RB': 1257,\n",
       " 'VBD VB JJ': 2964,\n",
       " 'VBZ IN JJ': 3627,\n",
       " 'VBG WRB': 3166,\n",
       " 'NN CC VBP': 1128,\n",
       " 'NNS VBP MD': 1727,\n",
       " 'JJS RB': 1058,\n",
       " 'VBG TO': 3134,\n",
       " 'WP VBP NN': 3941,\n",
       " 'VBG JJR': 3057,\n",
       " 'CC VBP DT': 153,\n",
       " 'IN CD NN': 494,\n",
       " 'FW FW NN': 467,\n",
       " 'WDT VB': 3833,\n",
       " 'JJ VBD TO': 924,\n",
       " 'PRP RB CC': 1918,\n",
       " 'DT WP VBP': 439,\n",
       " 'VBD DT JJS': 2820,\n",
       " 'TO NN CC': 2471,\n",
       " 'VB TO NN': 2710,\n",
       " 'VBP PRP WRB': 3458,\n",
       " 'RP PRP VBD': 2417,\n",
       " 'VBP VBN TO': 3550,\n",
       " 'RB CC NN': 2073,\n",
       " 'WRB JJ NN': 3982,\n",
       " 'VBP CC VBP': 3319,\n",
       " 'VB NN VBG': 2629,\n",
       " 'RB RBR IN': 2209,\n",
       " 'PRP VB RP': 1962,\n",
       " 'NN VB WRB': 1370,\n",
       " 'CC RB NNS': 97,\n",
       " 'VBD VBG RB': 2984,\n",
       " 'NNS CD NN': 1545,\n",
       " 'CC DT VBZ': 14,\n",
       " 'JJ IN VBZ': 775,\n",
       " 'VB VB TO': 2724,\n",
       " 'IN PDT DT': 618,\n",
       " 'NN VB PRP': 1361,\n",
       " 'VB RB VBZ': 2693,\n",
       " 'VBP CC PRP': 3316,\n",
       " 'VBP VBP DT': 3556,\n",
       " 'RB PRP RB': 2181,\n",
       " 'WP VBP JJ': 3940,\n",
       " 'VBD RP VB': 2950,\n",
       " 'CC VB NN': 116,\n",
       " 'VBP VBD': 3518,\n",
       " 'CC DT': 4,\n",
       " 'VBG RP VBG': 3133,\n",
       " 'VBG DT': 3019,\n",
       " 'VBD JJ PRP': 2851,\n",
       " 'JJ NN DT': 806,\n",
       " 'DT JJR NNS': 278,\n",
       " 'WP JJ VBP': 3891,\n",
       " 'NN RP CC': 1334,\n",
       " 'VBD NNS TO': 2895,\n",
       " 'RB PRP VBP': 2186,\n",
       " 'NN NNS RBR': 1270,\n",
       " 'VB PRP NNS': 2663,\n",
       " 'VBG JJ PRP': 3048,\n",
       " 'TO VBG NN': 2521,\n",
       " 'JJ JJ IN': 780,\n",
       " 'PRP VBG RP': 1997,\n",
       " 'VBP DT CC': 3326,\n",
       " 'RP IN PDT': 2382,\n",
       " 'VB VBD PRP': 2737,\n",
       " 'NN VBP WP': 1448,\n",
       " 'IN DT JJ': 501,\n",
       " 'VBN VBG NN': 3292,\n",
       " 'VBG NN IN': 3063,\n",
       " 'NN RB VBD': 1318,\n",
       " 'VBD': 2801,\n",
       " 'CD NN PRP': 204,\n",
       " 'TO CC': 2453,\n",
       " 'NN PRP VBZ': 1302,\n",
       " 'RB RB NN': 2195,\n",
       " 'RB EX VB': 2100,\n",
       " 'JJ TO RB': 896,\n",
       " 'VBD TO JJ': 2955,\n",
       " 'PRP RB': 1917,\n",
       " 'JJ RB RB': 879,\n",
       " 'VBP JJ RB': 3381,\n",
       " 'CC VB VBN': 125,\n",
       " 'NN VBN RP': 1421,\n",
       " 'IN NNS VBN': 611,\n",
       " 'JJR JJ NNS': 1005,\n",
       " 'CC NN VBD': 56,\n",
       " 'IN VBZ DT': 707,\n",
       " 'NNS IN VBP': 1567,\n",
       " 'VBN VBN NN': 3299,\n",
       " 'NN PRP': 1284,\n",
       " 'NNS VBD': 1684,\n",
       " 'PRP JJR NN': 1847,\n",
       " 'VBZ VBG VBN': 3766,\n",
       " 'VBG RP RB': 3131,\n",
       " 'VBG JJ IN': 3044,\n",
       " 'VBN JJ PRP': 3206,\n",
       " 'VBP VB VBP': 3514,\n",
       " 'VBP VBN JJ': 3544,\n",
       " 'VBP JJ VBG': 3385,\n",
       " 'NN IN MD': 1172,\n",
       " 'VB RP JJ': 2700,\n",
       " 'WRB RB VB': 4037,\n",
       " 'PRP VBD JJ': 1975,\n",
       " 'WDT JJ': 3814,\n",
       " 'VB NNS PRP': 2643,\n",
       " 'MD RB RB': 1087,\n",
       " 'VBG RBR': 3122,\n",
       " 'PRP MD RB': 1857,\n",
       " 'VB RB RB': 2685,\n",
       " 'WRB DT NN': 3969,\n",
       " 'VBG VBG DT': 3150,\n",
       " 'IN VBP JJ': 702,\n",
       " 'NN IN VBZ': 1185,\n",
       " 'VBD VBN IN': 2990,\n",
       " 'VB MD VB': 2615,\n",
       " 'PRP NN VB': 1872,\n",
       " 'DT NN WDT': 312,\n",
       " 'WDT VBD VB': 3845,\n",
       " 'WRB NN NNS': 4001,\n",
       " 'CC NN PRP': 52,\n",
       " 'CC PRP NNS': 82,\n",
       " 'PRP RBR': 1935,\n",
       " 'NN CC VBZ': 1129,\n",
       " 'DT NN VBP': 310,\n",
       " 'VBD VBN NNS': 2993,\n",
       " 'RB VBP RB': 2291,\n",
       " 'IN NNS VB': 608,\n",
       " 'JJ JJ VBG': 789,\n",
       " 'CC VBN': 149,\n",
       " 'VBG NN VBG': 3073,\n",
       " 'WRB VBZ IN': 4057,\n",
       " 'VB NN JJ': 2620,\n",
       " 'CC VBD IN': 131,\n",
       " 'DT VBG RP': 400,\n",
       " 'WP VBP PRP': 3943,\n",
       " 'VBD IN EX': 2832,\n",
       " 'RB JJ WP': 2135,\n",
       " 'VBZ PRP': 3687,\n",
       " 'PDT IN': 1777,\n",
       " 'VBP VBG JJ': 3529,\n",
       " 'VBP DT NNS': 3334,\n",
       " 'IN NN MD': 579,\n",
       " 'MD VB PRP': 1100,\n",
       " 'NNS JJ CC': 1571,\n",
       " 'NN CC DT': 1114,\n",
       " 'JJS NN VBD': 1050,\n",
       " 'VBG NN DT': 3061,\n",
       " 'DT CD': 235,\n",
       " 'PRP RB MD': 1922,\n",
       " 'VBD VBN VBN': 2998,\n",
       " 'DT VB JJ': 377,\n",
       " 'IN NNS WP': 615,\n",
       " 'WRB TO VB': 4040,\n",
       " 'NNS WDT VB': 1756,\n",
       " 'VBD NN IN': 2866,\n",
       " 'PRP VBD TO': 1982,\n",
       " 'VBG DT NN': 3023,\n",
       " 'RB VB RP': 2231,\n",
       " 'TO PRP': 2484,\n",
       " 'WP VBZ WRB': 3965,\n",
       " 'IN CC PRP': 486,\n",
       " 'DT CC': 229,\n",
       " 'DT RB JJ': 351,\n",
       " 'VBP PRP RB': 3449,\n",
       " 'TO PRP NNS': 2489,\n",
       " 'VB IN IN': 2575,\n",
       " 'NN VBN PRP': 1419,\n",
       " 'VBZ JJR': 3651,\n",
       " 'PDT DT JJ': 1773,\n",
       " 'IN VBD DT': 674,\n",
       " 'JJ VBG DT': 927,\n",
       " 'RBS RB': 2364,\n",
       " 'VBD VB RB': 2967,\n",
       " 'VBN NNS CC': 3228,\n",
       " 'VBZ NN VBZ': 3673,\n",
       " 'NN VBG VBD': 1408,\n",
       " 'FW DT': 462,\n",
       " 'NN CC JJ': 1117,\n",
       " 'MD DT NN': 1068,\n",
       " 'VBP WRB NN': 3592,\n",
       " 'IN CC VB': 488,\n",
       " 'VBG NN VBD': 3072,\n",
       " 'JJ NN': 803,\n",
       " 'NN CC TO': 1123,\n",
       " 'PRP VBZ CC': 2037,\n",
       " 'CC PRP VB': 85,\n",
       " 'NNS WRB DT': 1767,\n",
       " 'RBS IN': 2357,\n",
       " 'RB WP VBZ': 2323,\n",
       " 'CC NNS NN': 67,\n",
       " 'IN JJ DT': 538,\n",
       " 'VBZ RBR JJ': 3724,\n",
       " 'VBZ DT IN': 3609,\n",
       " 'PRP VBN JJ': 2005,\n",
       " 'NN VBD RBR': 1384,\n",
       " 'VB WP VBZ': 2789,\n",
       " 'DT NN VBZ': 311,\n",
       " 'VBP RB VBN': 3472,\n",
       " 'CC VB VBG': 124,\n",
       " 'NN CC NN': 1119,\n",
       " 'JJ WRB NN': 981,\n",
       " 'PRP RP IN': 1939,\n",
       " 'VBZ VBZ PRP': 3793,\n",
       " 'DT VBP WRB': 419,\n",
       " 'RB VBN PRP': 2273,\n",
       " 'DT NN NN': 297,\n",
       " 'RB PDT DT': 2172,\n",
       " 'RP PDT DT': 2412,\n",
       " 'IN VBZ VBG': 713,\n",
       " 'JJ JJR VB': 796,\n",
       " 'TO VB JJ': 2502,\n",
       " 'WRB PRP JJ': 4022,\n",
       " 'RB VB VBN': 2236,\n",
       " 'WDT VB IN': 3834,\n",
       " 'NNS NN NNS': 1595,\n",
       " 'DT RB DT': 349,\n",
       " 'CC PRP JJ': 79,\n",
       " 'MD JJ': 1074,\n",
       " 'NN VBZ RP': 1464,\n",
       " 'JJ VBG JJ': 929,\n",
       " 'VBP DT RB': 3336,\n",
       " 'NNS IN IN': 1558,\n",
       " 'NNS VBG NNS': 1704,\n",
       " 'VB VBG NN': 2743,\n",
       " 'RBR DT': 2330,\n",
       " 'VBD VBN TO': 2997,\n",
       " 'VBP VBN DT': 3542,\n",
       " 'RB JJ VBZ': 2134,\n",
       " 'WP VBP VB': 3946,\n",
       " 'RBR IN JJ': 2334,\n",
       " 'NNS VB PRP': 1675,\n",
       " 'NNS VBD TO': 1694,\n",
       " 'VBP IN JJS': 3355,\n",
       " 'VBN VB JJ': 3280,\n",
       " 'VBN MD VB': 3212,\n",
       " 'PRP JJ CC': 1828,\n",
       " 'CC VB RP': 120,\n",
       " 'CC RB NN': 96,\n",
       " 'JJ NNS TO': 846,\n",
       " 'VBD IN PRP': 2837,\n",
       " 'VBD PRP RB': 2913,\n",
       " 'IN JJ': 536,\n",
       " 'RP CC VB': 2369,\n",
       " 'NN PRP CC': 1285,\n",
       " 'MD': 1063,\n",
       " 'NN JJ CC': 1189,\n",
       " 'VB DT JJR': 2554,\n",
       " 'VBP RB TO': 3468,\n",
       " 'NNS RBR': 1653,\n",
       " 'RBR NNS': 2344,\n",
       " 'VBP RBR IN': 3477,\n",
       " 'VBZ RB': 3705,\n",
       " 'NN DT RB': 1147,\n",
       " 'TO RB VB': 2495,\n",
       " 'JJS IN DT': 1034,\n",
       " 'IN WRB DT': 725,\n",
       " 'VBZ VBN VBG': 3779,\n",
       " 'WP NNS': 3902,\n",
       " 'VB VB': 2713,\n",
       " 'NN JJ IN': 1191,\n",
       " 'CD IN NN': 189,\n",
       " 'DT VBZ NNS': 426,\n",
       " 'NN NN VBD': 1244,\n",
       " 'VB JJ NN': 2593,\n",
       " 'VBZ CC PRP': 3601,\n",
       " 'NN IN CC': 1163,\n",
       " 'NNS RB NN': 1641,\n",
       " 'PRP RB IN': 1920,\n",
       " 'IN PRP WDT': 640,\n",
       " 'CC NN IN': 47,\n",
       " 'IN VBD RB': 677,\n",
       " 'VBZ JJR NNS': 3654,\n",
       " 'VBP DT JJR': 3331,\n",
       " 'VBP IN DT': 3350,\n",
       " 'RP VBD': 2437,\n",
       " 'DT CD NNS': 239,\n",
       " 'VBP RP WRB': 3489,\n",
       " 'IN WRB PRP': 729,\n",
       " 'NN CC WRB': 1131,\n",
       " 'RB VBP CC': 2282,\n",
       " 'RB PRP CC': 2174,\n",
       " 'NN VBZ': 1450,\n",
       " 'DT DT DT': 241,\n",
       " 'NNS PRP VBZ': 1634,\n",
       " 'VBD JJ RB': 2852,\n",
       " 'VBZ RP NNS': 3733,\n",
       " 'WRB PRP': 4018,\n",
       " 'VBP RP NN': 3483,\n",
       " 'RB NN CC': 2143,\n",
       " 'VBN JJ VB': 3208,\n",
       " 'NN VB': 1354,\n",
       " 'VB VBD JJ': 2734,\n",
       " 'VBG RP DT': 3125,\n",
       " 'CC VBD DT': 130,\n",
       " 'CC VBG NNS': 145,\n",
       " 'VBP MD RB': 3397,\n",
       " 'VBN TO DT': 3271,\n",
       " 'IN WP VBD': 722,\n",
       " 'VB VBP IN': 2766,\n",
       " 'NNP NN JJ': 1514,\n",
       " 'DT RBR NNS': 365,\n",
       " 'NN VBD DT': 1374,\n",
       " 'JJR VB PRP': 1026,\n",
       " 'VBP VBP VB': 3565,\n",
       " 'VBZ JJ CC': 3637,\n",
       " 'JJ NNS PRP': 842,\n",
       " 'DT NN NNS': 299,\n",
       " 'VBN RP RB': 3269,\n",
       " 'PRP DT VBG': 1805,\n",
       " 'NN NNP PRP': 1256,\n",
       " 'VBP VBD NNS': 3523,\n",
       " 'VBG RB DT': 3114,\n",
       " 'IN JJ FW': 539,\n",
       " 'PRP PDT DT': 1902,\n",
       " 'NNS DT JJ': 1548,\n",
       " 'NNP VBP': 1522,\n",
       " 'WP RB VBD': 3913,\n",
       " 'DT JJ JJ': 258,\n",
       " 'NNS VBG': 1698,\n",
       " 'VB MD': 2613,\n",
       " 'WDT CD IN': 3812,\n",
       " 'TO PRP RB': 2491,\n",
       " 'CD NNS JJ': 214,\n",
       " 'NN UH NN': 1353,\n",
       " 'VB IN PRP': 2579,\n",
       " 'DT IN': 246,\n",
       " 'CC WDT': 172,\n",
       " 'VBG NNS VBZ': 3091,\n",
       " 'NN CC RB': 1122,\n",
       " 'NN RP': 1333,\n",
       " 'VB VBG CC': 2739,\n",
       " 'VBN VB DT': 3278,\n",
       " 'VBD CC IN': 2804,\n",
       " 'NNS VB TO': 1678,\n",
       " 'IN DT RB': 507,\n",
       " 'VB VBP RB': 2771,\n",
       " 'WP VBZ VBG': 3962,\n",
       " 'VBG PRP MD': 3100,\n",
       " 'IN JJ PRP': 546,\n",
       " 'VBP WRB PRP': 3594,\n",
       " 'VBG NNS TO': 3087,\n",
       " 'TO NN VBZ': 2478,\n",
       " 'VBN VBG JJ': 3291,\n",
       " 'IN DT NNS': 505,\n",
       " 'RB RB CC': 2189,\n",
       " 'RB IN NNS': 2107,\n",
       " 'JJR CC': 988,\n",
       " 'NN IN FW': 1167,\n",
       " 'VBP JJ JJ': 3375,\n",
       " 'PRP VBP VBZ': 2033,\n",
       " 'PRP DT CD': 1794,\n",
       " 'RP WRB PRP': 2451,\n",
       " 'NN VBD JJR': 1377,\n",
       " 'PRP RB VBD': 1930,\n",
       " 'VBP VBN': 3540,\n",
       " 'NN VB JJ': 1358,\n",
       " 'VB VBP VB': 2773,\n",
       " 'PRP VBD IN': 1974,\n",
       " 'NN VBZ CD': 1452,\n",
       " 'JJ NN CC': 804,\n",
       " 'CC VBP RB': 159,\n",
       " 'TO VB VBD': 2512,\n",
       " 'WP VB VBP': 3924,\n",
       " 'VBG VBN RP': 3160,\n",
       " ...}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 7086)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 4063)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 17)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.3 ,  79.94,  30.  , ...,   1.  ,   0.  ,   0.  ],\n",
       "       [  4.7 ,  90.13,  19.  , ...,   1.  ,   0.  ,   0.  ],\n",
       "       [  5.8 ,  89.25,  23.  , ...,   2.  ,   0.  ,   1.  ],\n",
       "       ...,\n",
       "       [  3.1 ,  96.03,  15.  , ...,   0.  ,   0.  ,   0.  ],\n",
       "       [  0.6 , 103.05,   8.  , ...,   0.  ,   0.  ,   0.  ],\n",
       "       [  9.8 ,  55.22,  27.  , ...,   0.  ,   1.  ,   0.  ]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 11166)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11166"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "# for k,v in vocab.iteritems():\n",
    "#     variables[v] = k\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "# for k,v in pos_vocab.iteritems():\n",
    "#     pos_variables[v] = k\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "This model was found using a GridSearch with 5-fold cross validation. Details are in the notebook operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 175)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.55      0.51      1430\n",
      "           1       0.97      0.92      0.94     19190\n",
      "           2       0.82      0.96      0.89      4163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     24783\n",
      "   macro avg       0.75      0.81      0.78     24783\n",
      "weighted avg       0.91      0.90      0.91     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using information from the model to obtain the matrix X_ generically\n",
    "\n",
    "This is the most difficult task: We have to take the inputs tweets and transform them into a format that can be used in the model without going through all the same pre-processing steps as above. This can be done as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = select.get_support(indices=True) #get indices of features\n",
    "# final_feature_list = [unicode(feature_names[i]) for i in final_features] #Get list of names corresponding to indices\n",
    "final_feature_list = [str(feature_names[i]) for i in final_features] #Get list of names corresponding to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['america', 'american', 'anoth', 'ass', 'ass cracker', 'ass hoe', 'ass nigga', 'bad', 'beaner', 'big', 'bird', 'bitch', 'bitch nigga', 'black', 'border', 'born', 'bout', 'browni', 'campu', 'charli', 'chink', 'color', 'color folk', 'coon', 'countri', 'cracker', 'crazi', 'crippl', 'cunt', 'da', 'damn', 'darki', 'dick', 'die', 'doe', 'dyke', 'fag', 'faggot', 'fat', 'femal', 'feminist', 'filth', 'first', 'folk', 'fucc nicca', 'fuck', 'fuckin', 'game', 'gay', 'get', 'ghetto', 'girl', 'gon', 'good', 'gook', 'got nigga', 'gt gt', 'hate', 'hate hoe', 'hi', 'hire', 'ho', 'hoe', 'hood', 'hope', 'human', 'israel', 'jap', 'jew', 'jihadi', 'kill', 'lame', 'latina', 'let', 'like', 'lol', 'look like', 'love', 'may', 'mexican', 'mock', 'money', 'monkey', 'much', 'muslim', 'muzzi', 'negro', 'nicca', 'nig', 'nigga', 'nigga bitch', 'niggah', 'niggaz', 'nigger', 'nigguh', 'niglet', 'oreo', 'peopl', 'play', 'pussi', 'queer', 'race', 'racist', 'real', 'real nigga', 'redneck', 'retard', 'sex', 'shit', 'shoot', 'show', 'shut', 'side', 'slope', 'smh', 'sole', 'special', 'spic', 'stfu', 'stupid', 'suck', 'tcot', 'teabagg', 'teach', 'thank', 'thi pussi', 'tho', 'throat', 'towel', 'trailer', 'tranni', 'trash', 'tri', 'twat', 'u', 'ugli', 'uncl', 'ur', 'us', 'use', 'wassup', 'watch', 'wear', 'wetback', 'whi', 'white', 'white trash', 'whitey', 'whore', 'word', 'ya', 'yanke', 'yeah', 'yellow', 'yr', 'zimmerman', 'IN NN', 'JJ NN', 'NN NN NN', 'NNS', 'PRP VBP', 'RB', 'VB', 'VBD', 'FKRA', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader compound', 'num_hashtags', 'num_mentions']\n"
     ]
    }
   ],
   "source": [
    "# print final_feature_list\n",
    "print(final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting names for each class of features\n",
    "ngram_features = final_feature_list[:final_feature_list.index('zimmerman')+1]\n",
    "pos_features = final_feature_list[final_feature_list.index('zimmerman')+1:final_feature_list.index('VBD')+1]\n",
    "oth_features = final_feature_list[final_feature_list.index('VBD')+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FKRA',\n",
       " 'FRE',\n",
       " 'num_syllables',\n",
       " 'num_chars',\n",
       " 'num_chars_total',\n",
       " 'num_terms',\n",
       " 'num_words',\n",
       " 'num_unique_words',\n",
       " 'vader compound',\n",
       " 'num_hashtags',\n",
       " 'num_mentions']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oth_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {v:i for i, v in enumerate(ngram_features)}\n",
    "new_vocab_to_index = {}\n",
    "for k in ngram_features:\n",
    "    new_vocab_to_index[k] = vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'america': 126,\n",
       " 'american': 127,\n",
       " 'anoth': 180,\n",
       " 'ass': 250,\n",
       " 'ass cracker': 260,\n",
       " 'ass hoe': 269,\n",
       " 'ass nigga': 277,\n",
       " 'bad': 356,\n",
       " 'beaner': 416,\n",
       " 'big': 493,\n",
       " 'bird': 512,\n",
       " 'bitch': 535,\n",
       " 'bitch nigga': 753,\n",
       " 'black': 893,\n",
       " 'border': 958,\n",
       " 'born': 960,\n",
       " 'bout': 973,\n",
       " 'browni': 1047,\n",
       " 'campu': 1132,\n",
       " 'charli': 1209,\n",
       " 'chink': 1253,\n",
       " 'color': 1332,\n",
       " 'color folk': 1335,\n",
       " 'coon': 1408,\n",
       " 'countri': 1433,\n",
       " 'cracker': 1444,\n",
       " 'crazi': 1450,\n",
       " 'crippl': 1470,\n",
       " 'cunt': 1499,\n",
       " 'da': 1525,\n",
       " 'damn': 1544,\n",
       " 'darki': 1561,\n",
       " 'dick': 1665,\n",
       " 'die': 1674,\n",
       " 'doe': 1721,\n",
       " 'dyke': 1820,\n",
       " 'fag': 2000,\n",
       " 'faggot': 2002,\n",
       " 'fat': 2040,\n",
       " 'femal': 2081,\n",
       " 'feminist': 2084,\n",
       " 'filth': 2099,\n",
       " 'first': 2117,\n",
       " 'folk': 2168,\n",
       " 'fucc nicca': 2245,\n",
       " 'fuck': 2246,\n",
       " 'fuckin': 2332,\n",
       " 'game': 2366,\n",
       " 'gay': 2379,\n",
       " 'get': 2390,\n",
       " 'ghetto': 2470,\n",
       " 'girl': 2485,\n",
       " 'gon': 2580,\n",
       " 'good': 2599,\n",
       " 'gook': 2620,\n",
       " 'got nigga': 2658,\n",
       " 'gt gt': 2728,\n",
       " 'hate': 2801,\n",
       " 'hate hoe': 2809,\n",
       " 'hi': 2859,\n",
       " 'hire': 2898,\n",
       " 'ho': 2911,\n",
       " 'hoe': 2916,\n",
       " 'hood': 3085,\n",
       " 'hope': 3094,\n",
       " 'human': 3128,\n",
       " 'israel': 3275,\n",
       " 'jap': 3298,\n",
       " 'jew': 3317,\n",
       " 'jihadi': 3323,\n",
       " 'kill': 3399,\n",
       " 'lame': 3486,\n",
       " 'latina': 3511,\n",
       " 'let': 3554,\n",
       " 'like': 3588,\n",
       " 'lol': 3756,\n",
       " 'look like': 3787,\n",
       " 'love': 3816,\n",
       " 'may': 3986,\n",
       " 'mexican': 4026,\n",
       " 'mock': 4095,\n",
       " 'money': 4109,\n",
       " 'monkey': 4115,\n",
       " 'much': 4149,\n",
       " 'muslim': 4165,\n",
       " 'muzzi': 4172,\n",
       " 'negro': 4237,\n",
       " 'nicca': 4291,\n",
       " 'nig': 4304,\n",
       " 'nigga': 4307,\n",
       " 'nigga bitch': 4314,\n",
       " 'niggah': 4386,\n",
       " 'niggaz': 4392,\n",
       " 'nigger': 4394,\n",
       " 'nigguh': 4401,\n",
       " 'niglet': 4405,\n",
       " 'oreo': 4573,\n",
       " 'peopl': 4653,\n",
       " 'play': 4735,\n",
       " 'pussi': 4901,\n",
       " 'queer': 4988,\n",
       " 'race': 5002,\n",
       " 'racist': 5006,\n",
       " 'real': 5050,\n",
       " 'real nigga': 5054,\n",
       " 'redneck': 5095,\n",
       " 'retard': 5148,\n",
       " 'sex': 5399,\n",
       " 'shit': 5430,\n",
       " 'shoot': 5467,\n",
       " 'show': 5478,\n",
       " 'shut': 5484,\n",
       " 'side': 5497,\n",
       " 'slope': 5564,\n",
       " 'smh': 5585,\n",
       " 'sole': 5624,\n",
       " 'special': 5689,\n",
       " 'spic': 5698,\n",
       " 'stfu': 5762,\n",
       " 'stupid': 5842,\n",
       " 'suck': 5858,\n",
       " 'tcot': 5987,\n",
       " 'teabagg': 5993,\n",
       " 'teach': 5995,\n",
       " 'thank': 6054,\n",
       " 'thi pussi': 6124,\n",
       " 'tho': 6183,\n",
       " 'throat': 6202,\n",
       " 'towel': 6289,\n",
       " 'trailer': 6295,\n",
       " 'tranni': 6300,\n",
       " 'trash': 6305,\n",
       " 'tri': 6335,\n",
       " 'twat': 6397,\n",
       " 'u': 6441,\n",
       " 'ugli': 6507,\n",
       " 'uncl': 6524,\n",
       " 'ur': 6548,\n",
       " 'us': 6558,\n",
       " 'use': 6563,\n",
       " 'wassup': 6718,\n",
       " 'watch': 6722,\n",
       " 'wear': 6743,\n",
       " 'wetback': 6779,\n",
       " 'whi': 6786,\n",
       " 'white': 6818,\n",
       " 'white trash': 6828,\n",
       " 'whitey': 6830,\n",
       " 'whore': 6838,\n",
       " 'word': 6908,\n",
       " 'ya': 6976,\n",
       " 'yanke': 6998,\n",
       " 'yeah': 7017,\n",
       " 'yellow': 7029,\n",
       " 'yr': 7074,\n",
       " 'zimmerman': 7083}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of text features\n",
    "ngram_indices = final_features[:len(ngram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle new vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_tfidf.pkl']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(new_vectorizer, 'final_tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf_ = new_vectorizer.fit_transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 4.80981477, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.81738461, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.627199378751962"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are the same if use IDF but the problem is that IDF will be different if we use different data. Instead we have to use the original IDF scores and multiply them by the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vals_ = idf_vals[ngram_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_idf.pkl']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Pickle idf_vals\n",
    "\n",
    "joblib.dump(idf_vals_, 'final_idf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_[1,:]*idf_vals_) == X_[1,:153] #Got same value as final process array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_*idf_vals_ == X_[:,:153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffinal = tfidf_*idf_vals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating POS features\n",
    "This is simpler as we do not need to worry about IDF but it will be slower as we have to compute the POS tags for the new data. Here we can simply use the old POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = {v:i for i, v in enumerate(pos_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle pos vectorizer\n",
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "new_pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_pos.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_pos_vectorizer, 'final_pos.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = new_pos_vectorizer.fit_transform(tweet_tags).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:] == X_[:,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95689.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33411.363676851324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[:,153:159].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can look at the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neg', 'vader pos', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls', 'is_retweet']\n"
     ]
    }
   ],
   "source": [
    "# print other_features_names\n",
    "print(other_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader compound', 'num_hashtags', 'num_mentions']\n"
     ]
    }
   ],
   "source": [
    "# print oth_features\n",
    "print(oth_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions can be modified to only calculate and return necessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array_(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_ = get_feature_array_(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3   ,  79.94  ,  30.    , 127.    , 140.    ,  25.    ,\n",
       "        25.    ,  23.    ,   0.4563,   0.    ,   1.    ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.    ,   1.    ,   2.    ,   1.    ,   0.    ,   8.3   ,\n",
       "        79.94  ,  30.    , 127.    , 140.    ,  25.    ,  25.    ,\n",
       "        23.    ,   0.4563,   0.    ,   1.    ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,159:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[:,:] == X_[:,159:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have put it all together using a simplified process we can assess if these new data return the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.concatenate([tfidffinal, pos_, feats_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 175)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = pd.DataFrame(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_ = model.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.56      0.51      1430\n",
      "           1       0.97      0.92      0.94     19190\n",
      "           2       0.82      0.96      0.89      4163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     24783\n",
      "   macro avg       0.75      0.81      0.78     24783\n",
      "weighted avg       0.91      0.90      0.91     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So now that we have verified that the results are the same with X_ and X__ we can implement a script that can transform new data in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
